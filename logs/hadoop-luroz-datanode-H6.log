2019-09-11 15:41:52,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-11 15:41:52,680 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-11 15:41:52,979 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-11 15:41:53,362 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-11 15:41:53,413 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-11 15:41:53,413 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-11 15:41:53,432 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-11 15:41:53,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-11 15:41:53,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-11 15:41:53,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-11 15:41:53,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-11 15:41:53,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-11 15:41:53,543 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-11 15:41:53,549 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-11 15:41:53,562 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-11 15:41:53,568 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-11 15:41:53,569 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-11 15:41:53,569 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-11 15:41:53,569 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-11 15:41:53,591 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42065
2019-09-11 15:41:53,591 INFO org.mortbay.log: jetty-6.1.26
2019-09-11 15:41:53,717 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42065
2019-09-11 15:41:54,062 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-11 15:41:54,064 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-11 15:41:54,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-11 15:41:54,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-11 15:41:54,169 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-11 15:41:54,184 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-11 15:41:54,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-11 15:41:54,375 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-11 15:41:54,382 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-11 15:41:54,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-11 15:41:54,426 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-11 15:41:54,426 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-11 15:41:55,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:41:56,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:41:57,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:41:58,573 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:41:59,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:00,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:01,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:02,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:03,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:04,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:04,579 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:42:04,583 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:42:10,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:11,585 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:12,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:13,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:14,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:15,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:16,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:17,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:18,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:19,591 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:19,592 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:42:19,593 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:42:25,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:26,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:27,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:28,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:29,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:30,597 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:31,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:32,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:33,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:34,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:34,601 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:42:34,602 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:42:40,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:41,604 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:42,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:43,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:44,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:45,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:46,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:47,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:48,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:49,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:49,610 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:42:49,611 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:42:55,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:56,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:57,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:58,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:42:59,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:00,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:01,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:02,617 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:03,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:04,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:04,619 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:43:04,620 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:43:10,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:11,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:12,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:13,623 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:14,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:15,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:16,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:17,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:18,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:19,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:19,627 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:43:19,628 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:43:25,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:26,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:27,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:28,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:29,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:30,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:31,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:32,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:33,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:34,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:34,635 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:43:34,636 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:43:40,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:41,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:42,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:43,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:44,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:45,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:46,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:47,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:48,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:49,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:49,644 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:43:49,645 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:43:55,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:56,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:57,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:58,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:43:59,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:00,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:01,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:02,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:03,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:04,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:04,653 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:44:04,653 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:44:10,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:11,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:12,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:13,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:14,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:15,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:16,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:17,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:18,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:19,660 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:19,660 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:44:19,661 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:44:25,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:26,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:27,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:28,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:29,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:30,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:31,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:32,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:33,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:34,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:34,669 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:44:34,670 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:44:40,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:41,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:42,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:43,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:44,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:45,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:46,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:47,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:48,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:49,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:49,679 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:44:49,680 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:44:55,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:56,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:57,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:58,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:44:59,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:00,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:01,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:02,685 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:03,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:04,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:04,687 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:45:04,687 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:45:10,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:11,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:12,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:13,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:14,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:15,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:16,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:17,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:18,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:19,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:19,695 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:45:19,696 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:45:25,697 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:26,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:27,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:28,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:29,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:30,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:31,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:32,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:33,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:34,703 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:34,703 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:45:34,704 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:45:40,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:41,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:42,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:43,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:44,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:45,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:46,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:47,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:48,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:49,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:49,712 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:45:49,713 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:45:55,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:56,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:57,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:58,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:45:59,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:00,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:01,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:02,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:03,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:04,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:04,721 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:46:04,722 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:46:10,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:11,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:12,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:13,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:14,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:15,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:16,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:17,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:18,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:19,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:19,730 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:46:19,731 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:46:25,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:26,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:27,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:28,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:29,734 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:30,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:31,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:32,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:33,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:34,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:34,738 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:46:34,739 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:46:40,740 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:41,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:42,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:43,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:44,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:45,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:46,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:47,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:48,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:49,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:49,747 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:46:49,748 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:46:55,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:56,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:57,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:58,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:46:59,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:00,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:01,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:02,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:03,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:04,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:04,756 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:47:04,757 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:47:10,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:11,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:12,759 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:13,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:14,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:15,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:16,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:17,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:18,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:19,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:19,764 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:47:19,765 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:47:25,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:26,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:27,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:28,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:29,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:30,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:31,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:32,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:33,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:34,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:34,773 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:47:34,774 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:47:40,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:41,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:42,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:43,777 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:44,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:45,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:46,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:47,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:48,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:49,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:49,782 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:47:49,783 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:47:55,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:56,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:57,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:58,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:47:59,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:00,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:01,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:02,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:03,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:04,789 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:04,790 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:48:04,791 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:48:10,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:11,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:12,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:13,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:14,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:15,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:16,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:17,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:18,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:19,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:19,798 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:48:19,798 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:48:25,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:26,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:27,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:28,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:29,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:30,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:31,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:32,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:33,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:34,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:34,805 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:48:34,806 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:48:40,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:41,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:42,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:43,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:44,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:45,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:46,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:47,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:48,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:49,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:49,812 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:48:49,813 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:48:55,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:56,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:57,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:58,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:48:59,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:00,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:01,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:02,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:03,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:04,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:04,820 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:49:04,821 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:49:10,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:11,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:12,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:13,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:14,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:15,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:16,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:17,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:18,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:19,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:19,828 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:49:19,828 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:49:25,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:26,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:27,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:28,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:29,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:30,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:31,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:32,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:33,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:34,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:34,835 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:49:34,836 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:49:40,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:41,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:42,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:43,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:44,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:45,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:46,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:47,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:48,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:49,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:49,843 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:49:49,843 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:49:55,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:56,845 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:57,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:58,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:49:59,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:00,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:01,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:02,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:03,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:04,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:04,850 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:50:04,851 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:50:10,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:11,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:12,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:13,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:14,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:15,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:16,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:17,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:18,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:19,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:19,858 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:50:19,858 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:50:25,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:26,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:27,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:28,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:29,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:30,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:31,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:32,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:33,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:34,865 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:34,866 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:50:34,867 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:50:40,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:41,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:42,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:43,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:44,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:45,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:46,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:47,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:48,872 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:49,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:49,873 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:50:49,874 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:50:55,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:56,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:57,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:58,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:50:59,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:00,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:01,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:02,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:03,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:04,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:04,880 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:51:04,880 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:51:10,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:11,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:12,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:13,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:14,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:15,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:16,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:17,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:18,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:19,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:19,886 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:51:19,887 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:51:25,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:26,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:27,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:28,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:29,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:30,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:31,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:32,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:33,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:34,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:34,893 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:51:34,894 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:51:40,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:41,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:42,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:43,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:44,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:45,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:46,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:47,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:48,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:49,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:49,900 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:51:49,900 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:51:54,459 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.getActorInfoMap(BPServiceActor.java:163)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getBPServiceActorInfo(DataNode.java:2958)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:51:54,464 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getNamenodeAddresses(DataNode.java:2930)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:51:55,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:56,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:57,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:58,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:51:59,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:00,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:01,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:02,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:03,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:04,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:04,906 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:52:04,907 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:52:10,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:11,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:12,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:13,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:14,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:15,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:16,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:17,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:18,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:19,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:19,913 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:52:19,914 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:52:25,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:26,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:27,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:28,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:29,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:30,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:31,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:32,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:33,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:34,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:34,921 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:52:34,922 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:52:40,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:41,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:42,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:43,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:44,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:45,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:46,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:47,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:48,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:49,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:49,928 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:52:49,928 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:52:55,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:56,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:57,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:58,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:52:59,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:00,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:01,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:02,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:03,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:04,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:04,935 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:53:04,936 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:53:10,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:11,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:12,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:13,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:14,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:15,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:16,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:17,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:18,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:19,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:19,942 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:53:19,943 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:53:25,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:26,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:27,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:28,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:29,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:30,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:31,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:32,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:33,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:34,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:34,949 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:53:34,949 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:53:40,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:41,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:42,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:43,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:44,952 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:45,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:46,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:47,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:48,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:49,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:49,955 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:53:49,956 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:53:55,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:56,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:57,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:58,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:53:59,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:00,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:01,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:02,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:03,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:04,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:04,962 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:54:04,963 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:54:10,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:11,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:12,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:13,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:14,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:15,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:16,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:17,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:18,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:19,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:19,969 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:54:19,969 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:54:25,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:26,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:27,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:28,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:29,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:30,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:31,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:32,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:33,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:34,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:34,976 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:54:34,977 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:54:40,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:41,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:42,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:43,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:44,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:45,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:46,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:47,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:48,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:49,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:49,983 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:54:49,984 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:54:55,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:56,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:57,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:58,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:54:59,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:00,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:01,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:02,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:03,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:04,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:04,991 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:55:04,991 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:55:10,992 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:11,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:12,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:13,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:14,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:15,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:16,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:17,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:18,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:19,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:19,997 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:55:19,998 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:55:25,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:26,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:28,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:29,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:30,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:31,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:32,002 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:33,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:34,003 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:35,004 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:35,004 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:55:35,005 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:55:41,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:42,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:43,007 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:44,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:45,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:46,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:47,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:48,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:49,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:50,011 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:50,012 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:55:50,012 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:55:56,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:57,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:58,014 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:55:59,015 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:00,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:01,016 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:02,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:03,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:04,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:05,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:05,019 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:56:05,020 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:56:11,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:12,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:13,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:14,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:15,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:16,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:17,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:18,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:19,025 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:20,026 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:20,026 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:56:20,027 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:56:26,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:27,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:28,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:29,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:30,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:31,031 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:32,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:33,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:34,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:35,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:35,034 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:56:35,034 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:56:41,035 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:42,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:43,036 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:44,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:45,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:46,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:47,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:48,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:49,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:50,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:50,041 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:56:50,041 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:56:56,042 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:57,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:58,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:56:59,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:00,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:01,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:02,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:03,046 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:04,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:05,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:05,048 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:57:05,049 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:57:11,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:12,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:13,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:14,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:15,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:16,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:17,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:18,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:19,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:20,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:20,055 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:57:20,056 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:57:26,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:27,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:28,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:29,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:30,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:31,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:32,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:33,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:34,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:35,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:35,063 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:57:35,064 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:57:41,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:42,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:43,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:44,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:45,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:46,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:47,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:48,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:49,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:50,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:50,070 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:57:50,071 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:57:56,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:57,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:58,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:57:59,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:00,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:01,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:02,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:03,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:04,076 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:05,077 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:05,078 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:58:05,078 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:58:11,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:12,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:13,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:14,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:15,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:16,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:17,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:18,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:19,083 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:20,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:20,084 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:58:20,085 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:58:26,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:27,087 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:28,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:29,088 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:30,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:31,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:32,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:33,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:34,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:35,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:35,092 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:58:35,093 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:58:41,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:42,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:43,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:44,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:45,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:46,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:47,097 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:48,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:49,098 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:50,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:50,099 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:58:50,100 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:58:56,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:57,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:58,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:58:59,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:00,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:01,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:02,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:03,104 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:04,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:05,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:05,106 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:59:05,107 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:59:11,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:12,108 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:13,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:14,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:15,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:16,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:17,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:18,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:19,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:20,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:20,114 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:59:20,114 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:59:26,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:27,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:28,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:29,117 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:30,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:31,118 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:32,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:33,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:34,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:35,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:35,121 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:59:35,121 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:59:41,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:42,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:43,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:44,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:45,124 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:46,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:47,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:48,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:49,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:50,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:50,127 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 15:59:50,127 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 15:59:56,128 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:57,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:58,129 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 15:59:59,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:00,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:01,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:02,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:03,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:04,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:05,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:05,133 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:00:05,134 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:00:11,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:12,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:13,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:14,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:15,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:16,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:17,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:18,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:19,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:20,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:20,140 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:00:20,140 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:00:26,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:27,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:28,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:29,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:30,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:31,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:32,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:33,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:34,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:35,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:35,147 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:00:35,147 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:00:41,148 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:42,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:43,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:44,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:45,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:46,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:47,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:48,151 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:49,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:50,152 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:50,153 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:00:50,153 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:00:56,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:57,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:58,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:00:59,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:00,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:01,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:02,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:03,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:04,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:05,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:05,159 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:01:05,160 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:01:11,161 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:12,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:13,162 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:14,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:15,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:16,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:17,164 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:18,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:19,165 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:20,166 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:20,166 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:01:20,167 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:01:26,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:27,168 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:28,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:29,169 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:30,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:31,170 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:32,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:33,171 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:34,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:35,172 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:35,173 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:01:35,173 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:01:41,174 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:42,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:43,175 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:44,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:45,176 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:46,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:47,177 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:48,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:49,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:50,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:50,179 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:01:50,180 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:01:54,471 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.getActorInfoMap(BPServiceActor.java:163)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getBPServiceActorInfo(DataNode.java:2958)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:01:54,471 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getNamenodeAddresses(DataNode.java:2930)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:01:56,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:57,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:58,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:01:59,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:00,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:01,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:02,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:03,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:04,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:05,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:05,186 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:02:05,186 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:02:11,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:12,187 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:13,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:14,188 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:15,189 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:16,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:17,190 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:18,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:19,191 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:20,192 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:20,192 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:02:20,193 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:02:26,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:27,194 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:28,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:29,195 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:30,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:31,196 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:32,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:33,197 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:34,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:35,198 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:35,199 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:02:35,200 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:02:41,201 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:42,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:43,202 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:44,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:45,203 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:46,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:47,204 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:48,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:49,205 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:50,206 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:50,206 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:02:50,207 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:02:56,208 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:57,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:58,209 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:02:59,210 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:00,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:01,211 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:02,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:03,212 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:04,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:05,213 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:05,214 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:03:05,214 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:03:11,215 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:12,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:13,216 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:14,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:15,217 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:16,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:17,218 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:18,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:19,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:20,219 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:20,220 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:03:20,220 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:03:26,221 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:27,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:28,222 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:29,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:30,223 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:31,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:32,224 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:33,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:34,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:35,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:35,226 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:03:35,227 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:03:41,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:42,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:43,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:44,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:45,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:46,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:47,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:48,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:49,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:50,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:50,233 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:03:50,233 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:03:56,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:57,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:58,235 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:03:59,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:00,236 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:01,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:02,237 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:03,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:04,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:05,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:05,239 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:04:05,240 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:04:11,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:12,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:13,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:14,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:15,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:16,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:17,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:18,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:19,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:20,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:20,246 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:04:20,247 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:04:26,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:27,248 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:28,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:29,249 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:30,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:31,250 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:32,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:33,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:34,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:35,252 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:35,253 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:04:35,253 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:04:41,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:42,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:43,255 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:44,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:45,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:46,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:47,257 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:48,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:49,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:50,259 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:50,259 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:04:50,260 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:04:56,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:57,261 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:58,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:04:59,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:00,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:01,263 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:02,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:03,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:04,264 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:05,265 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:05,265 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:05:05,266 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:05:11,267 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:12,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:13,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:14,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:15,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:16,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:17,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:18,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:19,271 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:20,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:20,272 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:05:20,273 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:05:26,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:27,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:28,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:29,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:30,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:31,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:32,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:33,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:34,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:35,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:35,279 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:05:35,279 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:05:41,280 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:42,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:43,281 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:44,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:45,282 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:46,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:47,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:48,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:49,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:50,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:50,285 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:05:50,286 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:05:56,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:57,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:58,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:05:59,288 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:00,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:01,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:02,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:03,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:04,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:05,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:05,292 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:06:05,292 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:06:11,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:12,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:13,294 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:14,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:15,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:16,295 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:17,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:18,296 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:19,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:20,297 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:20,298 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:06:20,298 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:06:26,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:27,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:28,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:29,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:30,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:31,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:32,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:33,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:34,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:35,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:35,304 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:06:35,305 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:06:41,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:42,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:43,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:44,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:45,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:46,308 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:47,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:48,309 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:49,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:50,310 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:50,311 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:06:50,311 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:06:56,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:57,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:58,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:06:59,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:00,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:01,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:02,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:03,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:04,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:05,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:05,317 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:07:05,317 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:07:11,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:12,318 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:13,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:14,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:15,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:16,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:17,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:18,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:19,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:20,323 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:20,323 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:07:20,324 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:07:26,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:27,325 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:28,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:29,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:30,326 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:31,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:32,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:33,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:34,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:35,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:35,329 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:07:35,330 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:07:41,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:42,331 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:43,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:44,333 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:45,333 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:46,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:47,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:48,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:49,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:50,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:50,336 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:07:50,337 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:07:56,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:57,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:58,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:07:59,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:00,340 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:01,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:02,341 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:03,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:04,342 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:05,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:05,343 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:08:05,344 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:08:11,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:12,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:13,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:14,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:15,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:16,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:17,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:18,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:19,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:20,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:20,350 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:08:20,351 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:08:26,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:27,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:28,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:29,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:30,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:31,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:32,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:33,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:34,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:35,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:35,357 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:08:35,357 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:08:41,358 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:42,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:43,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:44,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:45,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:46,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:47,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:48,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:49,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:50,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:50,363 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:08:50,364 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:08:56,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:57,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:58,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:08:59,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:00,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:01,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:02,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:03,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:04,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:05,369 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:05,370 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:09:05,370 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:09:11,371 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:12,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:13,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:14,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:15,373 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:16,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:17,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:18,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:19,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:20,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:20,376 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:09:20,377 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:09:26,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:27,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:28,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:29,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:30,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:31,380 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:32,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:33,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:34,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:35,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:35,383 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:09:35,383 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:09:41,384 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:42,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:43,385 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:44,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:45,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:46,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:47,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:48,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:49,388 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:50,389 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:50,389 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:09:50,390 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:09:56,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:57,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:58,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:09:59,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:00,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:01,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:02,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:03,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:04,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:05,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:05,396 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:10:05,396 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:10:11,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:12,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:13,398 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:14,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:15,399 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:16,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:17,400 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:18,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:19,401 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:20,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:20,402 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:10:20,403 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:10:26,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:27,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:28,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:29,405 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:30,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:31,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:32,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:33,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:34,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:35,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:35,409 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:10:35,409 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:10:41,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:42,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:43,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:44,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:45,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:46,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:47,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:48,413 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:49,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:50,414 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:50,415 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:10:50,415 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:10:56,416 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:57,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:58,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:10:59,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:00,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:01,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:02,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:03,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:04,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:05,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:05,421 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:11:05,422 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:11:11,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:12,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:13,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:14,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:15,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:16,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:17,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:18,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:19,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:20,427 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:20,427 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:11:20,428 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:11:26,429 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:27,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:28,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:29,430 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:30,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:31,431 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:32,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:33,432 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:34,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:35,433 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:35,434 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:11:35,434 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:11:41,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:42,435 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:43,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:44,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:45,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:46,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:47,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:48,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:49,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:50,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:50,439 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:11:50,440 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:11:54,478 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.getActorInfoMap(BPServiceActor.java:163)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getBPServiceActorInfo(DataNode.java:2958)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:11:54,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getNamenodeAddresses(DataNode.java:2930)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:11:56,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:57,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:58,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:11:59,442 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:00,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:01,443 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:02,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:03,444 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:04,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:05,445 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:05,445 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:12:05,446 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:12:11,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:12,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:13,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:14,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:15,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:16,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:17,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:18,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:19,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:20,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:20,451 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:12:20,452 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:12:26,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:27,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:28,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:29,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:30,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:31,455 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:32,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:33,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:34,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:35,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:35,458 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:12:35,458 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:12:41,459 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:42,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:43,460 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:44,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:45,461 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:46,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:47,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:48,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:49,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:50,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:50,464 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:12:50,465 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:12:56,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:57,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:58,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:12:59,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:00,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:01,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:02,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:03,469 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:04,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:05,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:05,471 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:13:05,472 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:13:11,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:12,473 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:13,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:14,474 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:15,475 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:16,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:17,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:18,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:19,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:20,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:20,478 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:13:20,479 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:13:26,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:27,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:28,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:29,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:30,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:31,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:32,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:33,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:34,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:35,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:35,485 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:13:35,485 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:13:41,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:42,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:43,487 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:44,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:45,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:46,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:47,489 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:48,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:49,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:50,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:50,491 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:13:50,492 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:13:56,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:57,493 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:58,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:13:59,494 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:00,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:01,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:02,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:03,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:04,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:05,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:05,498 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:14:05,498 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:14:11,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:12,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:13,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:14,500 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:15,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:16,501 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:17,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:18,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:19,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:20,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:20,504 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:14:20,504 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:14:26,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:27,505 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:28,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:29,506 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:30,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:31,507 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:32,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:33,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:34,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:35,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:35,510 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:14:35,510 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:14:41,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:42,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:43,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:44,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:45,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:46,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:47,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:48,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:49,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:50,516 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:50,517 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:14:50,517 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:14:56,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:57,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:58,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:14:59,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:00,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:01,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:02,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:03,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:04,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:05,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:05,523 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:15:05,523 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:15:11,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:12,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:13,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:14,525 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:15,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:16,526 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:17,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:18,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:19,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:20,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:20,529 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:15:20,529 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:15:26,530 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:27,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:28,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:29,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:30,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:31,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:32,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:33,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:34,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:35,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:35,535 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:15:35,536 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:15:41,536 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:42,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:43,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:44,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:45,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:46,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:47,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:48,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:49,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:50,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:50,541 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:15:50,542 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:15:56,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:57,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:58,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:15:59,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:00,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:01,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:02,545 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:03,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:04,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:05,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:05,547 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:16:05,548 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:16:11,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:12,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:13,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:14,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:15,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:16,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:17,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:18,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:19,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:20,553 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:20,554 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:16:20,554 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:16:26,555 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:27,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:28,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:29,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:30,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:31,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:32,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:33,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:34,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:35,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:35,560 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:16:35,560 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:16:41,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:42,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:43,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:44,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:45,563 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:46,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:47,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:48,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:49,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:50,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:50,566 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:16:50,566 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:16:56,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:57,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:58,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:16:59,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:00,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:01,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:02,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:03,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:04,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:05,572 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:05,572 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:17:05,573 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:17:11,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:12,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:13,574 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:14,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:15,575 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:16,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:17,576 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:18,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:19,577 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:20,578 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:20,578 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:17:20,579 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:17:26,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:27,580 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:28,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:29,581 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:30,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:31,582 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:32,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:33,583 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:34,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:35,584 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:35,585 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:17:35,585 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:17:41,586 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:42,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:43,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:44,587 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:45,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:46,588 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:47,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:48,589 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:49,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:50,590 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:50,591 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:17:50,591 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:17:56,592 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:57,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:58,593 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:17:59,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:00,594 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:01,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:02,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:03,595 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:04,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:05,596 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:05,597 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:18:05,597 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:18:11,598 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:12,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:13,599 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:14,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:15,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:16,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:17,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:18,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:19,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:20,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:20,603 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:18:20,604 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:18:26,605 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:27,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:28,606 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:29,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:30,607 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:31,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:32,608 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:33,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:34,609 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:35,610 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:35,610 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:18:35,611 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:18:41,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:42,612 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:43,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:44,613 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:45,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:46,614 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:47,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:48,615 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:49,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:50,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:50,616 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:18:50,617 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:18:56,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:57,618 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:58,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:18:59,619 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:00,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:01,620 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:02,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:03,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:04,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:05,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:05,623 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:19:05,623 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:19:11,624 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:12,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:13,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:14,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:15,626 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:16,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:17,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:18,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:19,628 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:20,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:20,629 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:19:20,629 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:19:26,630 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:27,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:28,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:29,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:30,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:31,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:32,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:33,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:34,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:35,634 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:35,635 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:19:35,635 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:19:41,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:42,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:43,637 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:44,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:45,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:46,638 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:47,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:48,639 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:49,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:50,640 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:50,641 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:19:50,641 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:19:56,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:57,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:58,643 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:19:59,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:00,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:01,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:02,645 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:03,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:04,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:05,647 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:05,647 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:20:05,647 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:20:11,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:12,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:13,649 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:14,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:15,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:16,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:17,651 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:18,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:19,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:20,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:20,653 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:20:20,654 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:20:26,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:27,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:28,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:29,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:30,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:31,657 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:32,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:33,658 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:34,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:35,659 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:35,660 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:20:35,660 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:20:41,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:42,661 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:43,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:44,662 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:45,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:46,663 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:47,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:48,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:49,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:50,665 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:50,666 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:20:50,666 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:20:56,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:57,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:58,668 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:20:59,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:00,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:01,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:02,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:03,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:04,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:05,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:05,672 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:21:05,672 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:21:11,673 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:12,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:13,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:14,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:15,675 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:16,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:17,676 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:18,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:19,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:20,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:20,678 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:21:20,678 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:21:26,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:27,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:28,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:29,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:30,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:31,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:32,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:33,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:34,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:35,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:35,684 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:21:35,685 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:21:41,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:42,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:43,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:44,687 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:45,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:46,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:47,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:48,689 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:49,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:50,690 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:50,691 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:21:50,691 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:21:54,487 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.getActorInfoMap(BPServiceActor.java:163)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getBPServiceActorInfo(DataNode.java:2958)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:21:54,487 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getNamenodeAddresses(DataNode.java:2930)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:21:56,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:57,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:58,693 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:21:59,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:00,694 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:01,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:02,695 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:03,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:04,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:05,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:05,697 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:22:05,697 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:22:11,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:12,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:13,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:14,699 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:15,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:16,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:17,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:18,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:19,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:20,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:20,703 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:22:20,703 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:22:26,704 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:27,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:28,705 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:29,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:30,706 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:31,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:32,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:33,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:34,708 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:35,709 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:35,709 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:22:35,709 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:22:41,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:42,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:43,711 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:44,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:45,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:46,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:47,713 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:48,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:49,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:50,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:50,715 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:22:50,715 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:22:56,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:57,716 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:58,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:22:59,717 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:00,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:01,718 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:02,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:03,719 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:04,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:05,720 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:05,721 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:23:05,721 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:23:11,722 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:12,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:13,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:14,723 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:15,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:16,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:17,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:18,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:19,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:20,726 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:20,727 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:23:20,727 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:23:26,728 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:27,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:28,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:29,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:30,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:31,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:32,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:33,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:34,732 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:35,733 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:35,733 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:23:35,734 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:23:41,735 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:42,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:43,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:44,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:45,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:46,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:47,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:48,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:49,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:50,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:50,740 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:23:50,741 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:23:56,741 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:57,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:58,742 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:23:59,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:00,743 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:01,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:02,744 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:03,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:04,745 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:05,746 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:05,746 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:24:05,747 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:24:11,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:12,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:13,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:14,749 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:15,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:16,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:17,750 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:18,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:19,751 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:20,752 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:20,752 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:24:20,753 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:24:26,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:27,754 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:28,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:29,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:30,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:31,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:32,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:33,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:34,757 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:35,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:35,758 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:24:35,759 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:24:41,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:42,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:43,760 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:44,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:45,761 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:46,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:47,762 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:48,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:49,763 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:50,764 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:50,764 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:24:50,765 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:24:56,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:57,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:58,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:24:59,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:00,767 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:01,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:02,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:03,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:04,769 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:05,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:05,770 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:25:05,771 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:25:11,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:12,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:13,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:14,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:15,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:16,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:17,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:18,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:19,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:20,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:20,776 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:25:20,777 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:25:26,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:27,778 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:28,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:29,779 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:30,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:31,780 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:32,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:33,781 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:34,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:35,782 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:35,783 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:25:35,783 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:25:41,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:42,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:43,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:44,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:45,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:46,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:47,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:48,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:49,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:50,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:50,789 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:25:50,789 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:25:56,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:57,790 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:58,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:25:59,791 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:00,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:01,792 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:02,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:03,793 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:04,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:05,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:05,794 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:26:05,795 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:26:11,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:12,796 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:13,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:14,797 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:15,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:16,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:17,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:18,799 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:19,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:20,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:20,801 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:26:20,801 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:26:26,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:27,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:28,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:29,803 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:30,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:31,804 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:32,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:33,805 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:34,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:35,806 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:35,807 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:26:35,807 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:26:41,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:42,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:43,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:44,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:45,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:46,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:47,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:48,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:49,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:50,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:50,812 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:26:50,813 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:26:56,814 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:57,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:58,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:26:59,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:00,816 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:01,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:02,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:03,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:04,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:05,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:05,819 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:27:05,820 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:27:11,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:12,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:13,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:14,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:15,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:16,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:17,823 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:18,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:19,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:20,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:20,825 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:27:20,826 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:27:26,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:27,827 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:28,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:29,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:30,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:31,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:32,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:33,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:34,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:35,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:35,831 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:27:35,832 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:27:41,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:42,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:43,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:44,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:45,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:46,835 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:47,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:48,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:49,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:50,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:50,838 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:27:50,838 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:27:56,839 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:57,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:58,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:27:59,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:00,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:01,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:02,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:03,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:04,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:05,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:05,844 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:28:05,845 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:28:11,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:12,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:13,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:14,847 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:15,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:16,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:17,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:18,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:19,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:20,850 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:20,851 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:28:20,851 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:28:26,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:27,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:28,853 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:29,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:30,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:31,855 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:32,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:33,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:34,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:35,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:35,857 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:28:35,858 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:28:41,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:42,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:43,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:44,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:45,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:46,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:47,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:48,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:49,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:50,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:50,864 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:28:50,865 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:28:56,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:57,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:58,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:28:59,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:00,868 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:01,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:02,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:03,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:04,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:05,870 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:05,871 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:29:05,872 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:29:11,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:12,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:13,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:14,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:15,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:16,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:17,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:18,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:19,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:20,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:20,877 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:29:20,878 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:29:26,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:27,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:28,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:29,880 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:30,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:31,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:32,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:33,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:34,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:35,883 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:35,884 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:29:35,884 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:29:41,885 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:42,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:43,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:44,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:45,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:46,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:47,888 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:48,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:49,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:50,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:50,890 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:29:50,891 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:29:56,891 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:57,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:58,892 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:29:59,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:00,893 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:01,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:02,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:03,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:04,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:05,895 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:05,896 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:30:05,896 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:30:11,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:12,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:13,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:14,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:15,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:16,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:17,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:18,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:19,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:20,901 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:20,902 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:30:20,902 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:30:26,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:27,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:28,904 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:29,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:30,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:31,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:32,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:33,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:34,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:35,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:35,908 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:30:35,909 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:30:41,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:42,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:43,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:44,910 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:45,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:46,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:47,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:48,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:49,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:50,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:50,914 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:30:50,914 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:30:56,915 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:57,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:58,916 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:30:59,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:00,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:01,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:02,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:03,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:04,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:05,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:05,920 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:31:05,921 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:31:11,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:12,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:13,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:14,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:15,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:16,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:17,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:18,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:19,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:20,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:20,926 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:31:20,927 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:31:26,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:27,928 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:28,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:29,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:30,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:31,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:32,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:33,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:34,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:35,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:35,933 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:31:35,933 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:31:41,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:42,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:43,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:44,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:45,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:46,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:47,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:48,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:49,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:50,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:50,939 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:31:50,940 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:31:54,494 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.getActorInfoMap(BPServiceActor.java:163)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getBPServiceActorInfo(DataNode.java:2958)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:31:54,495 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.getNamenodeAddresses(DataNode.java:2930)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)
	at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)
	at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttributes(MBeanSupport.java:213)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705)
	at org.apache.hadoop.hdfs.server.common.MetricsLoggerTask.run(MetricsLoggerTask.java:91)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:31:56,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:57,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:58,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:31:59,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:00,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:01,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:02,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:03,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:04,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:05,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:05,945 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:32:05,946 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:32:11,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:12,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:13,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:14,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:15,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:16,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:17,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:18,950 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:19,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:20,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:20,952 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:32:20,952 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:32:26,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:27,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:28,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:29,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:30,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:31,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:32,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:33,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:34,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:35,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:35,958 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:32:35,958 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:32:41,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:42,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:43,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:44,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:45,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:46,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:47,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:48,962 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:49,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:50,963 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:50,964 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:32:50,965 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:32:56,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:57,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:58,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:32:59,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:00,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:01,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:02,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:03,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:04,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:05,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:05,970 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-11 16:33:05,971 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-09-11 16:33:11,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:12,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:13,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:14,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:15,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:16,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:17,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-11 16:33:18,458 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-11 16:33:18,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-11 16:59:46,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-11 16:59:46,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-11 16:59:46,421 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-11 16:59:46,707 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-11 16:59:46,759 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-11 16:59:46,759 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-11 16:59:46,764 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-11 16:59:46,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-11 16:59:46,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-11 16:59:46,786 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-11 16:59:46,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-11 16:59:46,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-11 16:59:46,865 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-11 16:59:46,871 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-11 16:59:46,884 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-11 16:59:46,889 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-11 16:59:46,891 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-11 16:59:46,891 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-11 16:59:46,891 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-11 16:59:46,905 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46409
2019-09-11 16:59:46,905 INFO org.mortbay.log: jetty-6.1.26
2019-09-11 16:59:47,017 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46409
2019-09-11 16:59:47,201 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-11 16:59:47,203 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-11 16:59:47,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-11 16:59:47,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-11 16:59:47,260 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-11 16:59:47,272 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-11 16:59:47,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-11 16:59:47,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-11 16:59:47,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-11 16:59:47,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-11 16:59:47,357 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-11 16:59:47,358 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-11 16:59:47,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-11 16:59:47,595 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-11 16:59:47,663 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 11686@H6
2019-09-11 16:59:47,664 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/luroz/hadoop/datanode is not formatted for namespace 614487484. Formatting...
2019-09-11 16:59:47,665 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-75dd5b70-7f52-470d-b183-cfb71cfeb5ef for directory /home/luroz/hadoop/datanode
2019-09-11 16:59:47,790 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-697312766-127.0.1.1-1568235506165
2019-09-11 16:59:47,791 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165
2019-09-11 16:59:47,791 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165 is not formatted for BP-697312766-127.0.1.1-1568235506165. Formatting ...
2019-09-11 16:59:47,791 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-697312766-127.0.1.1-1568235506165 directory /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current
2019-09-11 16:59:47,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=614487484;bpid=BP-697312766-127.0.1.1-1568235506165;lv=-57;nsInfo=lv=-63;cid=CID-51d3ad8d-f2a6-4911-b240-848f0634fc7a;nsid=614487484;c=1568235506165;bpid=BP-697312766-127.0.1.1-1568235506165;dnuuid=null
2019-09-11 16:59:47,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 7308bf1f-ab7e-44ae-beb0-e551d711e956
2019-09-11 16:59:47,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-75dd5b70-7f52-470d-b183-cfb71cfeb5ef
2019-09-11 16:59:47,988 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-09-11 16:59:47,991 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-11 16:59:47,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-11 16:59:47,996 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-697312766-127.0.1.1-1568235506165
2019-09-11 16:59:47,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-697312766-127.0.1.1-1568235506165 on volume /home/luroz/hadoop/datanode/current...
2019-09-11 16:59:48,037 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-697312766-127.0.1.1-1568235506165 on /home/luroz/hadoop/datanode/current: 40ms
2019-09-11 16:59:48,037 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-697312766-127.0.1.1-1568235506165: 41ms
2019-09-11 16:59:48,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-697312766-127.0.1.1-1568235506165 on volume /home/luroz/hadoop/datanode/current...
2019-09-11 16:59:48,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current/replicas doesn't exist 
2019-09-11 16:59:48,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-697312766-127.0.1.1-1568235506165 on volume /home/luroz/hadoop/datanode/current: 0ms
2019-09-11 16:59:48,040 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2019-09-11 16:59:48,042 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-697312766-127.0.1.1-1568235506165 on volume /home/luroz/hadoop/datanode
2019-09-11 16:59:48,050 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 9/11/19 7:25 PM with interval of 21600000ms
2019-09-11 16:59:48,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-697312766-127.0.1.1-1568235506165 (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-11 16:59:48,060 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-75dd5b70-7f52-470d-b183-cfb71cfeb5ef): finished scanning block pool BP-697312766-127.0.1.1-1568235506165
2019-09-11 16:59:48,086 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-75dd5b70-7f52-470d-b183-cfb71cfeb5ef): no suitable block pools found to scan.  Waiting 1814399956 ms.
2019-09-11 16:59:48,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-697312766-127.0.1.1-1568235506165 (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-11 16:59:48,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-11 16:59:48,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x85100926bd0a7e8b,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 40 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-11 16:59:48,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-697312766-127.0.1.1-1568235506165
2019-09-11 17:01:38,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-697312766-127.0.1.1-1568235506165:blk_1073741825_1001 src: /127.0.0.1:60130 dest: /127.0.0.1:50010
2019-09-11 17:01:38,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60130, dest: /127.0.0.1:50010, bytes: 301933, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1368566515_1, offset: 0, srvID: 7308bf1f-ab7e-44ae-beb0-e551d711e956, blockid: BP-697312766-127.0.1.1-1568235506165:blk_1073741825_1001, duration: 15750416
2019-09-11 17:01:38,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-697312766-127.0.1.1-1568235506165:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-09-11 17:01:42,162 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2019-09-11 17:01:42,163 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-697312766-127.0.1.1-1568235506165 blk_1073741825_1001 file /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current/finalized/subdir0/subdir0/blk_1073741825
2019-09-11 17:01:56,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-697312766-127.0.1.1-1568235506165:blk_1073741826_1002 src: /127.0.0.1:60136 dest: /127.0.0.1:50010
2019-09-11 17:01:56,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60136, dest: /127.0.0.1:50010, bytes: 301933, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1477225758_1, offset: 0, srvID: 7308bf1f-ab7e-44ae-beb0-e551d711e956, blockid: BP-697312766-127.0.1.1-1568235506165:blk_1073741826_1002, duration: 11949516
2019-09-11 17:01:56,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-697312766-127.0.1.1-1568235506165:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2019-09-11 17:02:00,107 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2019-09-11 17:02:00,107 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-697312766-127.0.1.1-1568235506165 blk_1073741826_1002 file /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current/finalized/subdir0/subdir0/blk_1073741826
2019-09-11 17:07:45,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-697312766-127.0.1.1-1568235506165:blk_1073741827_1003 src: /127.0.0.1:60184 dest: /127.0.0.1:50010
2019-09-11 17:07:45,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60184, dest: /127.0.0.1:50010, bytes: 301933, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1241684424_1, offset: 0, srvID: 7308bf1f-ab7e-44ae-beb0-e551d711e956, blockid: BP-697312766-127.0.1.1-1568235506165:blk_1073741827_1003, duration: 15918557
2019-09-11 17:07:45,848 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-697312766-127.0.1.1-1568235506165:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-09-11 17:07:48,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-09-11 17:07:48,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-697312766-127.0.1.1-1568235506165 blk_1073741827_1003 file /home/luroz/hadoop/datanode/current/BP-697312766-127.0.1.1-1568235506165/current/finalized/subdir0/subdir0/blk_1073741827
2019-09-12 09:25:48,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-12 09:25:48,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-12 09:25:49,182 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 09:25:49,512 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-12 09:25:49,562 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-12 09:25:49,562 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-12 09:25:49,567 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-12 09:25:49,568 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-12 09:25:49,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-12 09:25:49,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-12 09:25:49,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-12 09:25:49,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-12 09:25:49,681 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-12 09:25:49,688 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-12 09:25:49,701 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-12 09:25:49,706 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-12 09:25:49,708 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-12 09:25:49,708 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-12 09:25:49,708 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-12 09:25:49,721 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41023
2019-09-12 09:25:49,721 INFO org.mortbay.log: jetty-6.1.26
2019-09-12 09:25:49,837 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41023
2019-09-12 09:25:50,128 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-12 09:25:50,130 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-12 09:25:50,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-12 09:25:50,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-12 09:25:50,216 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-12 09:25:50,227 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-12 09:25:50,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-12 09:25:50,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-12 09:25:50,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-12 09:25:50,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-12 09:25:50,351 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-12 09:25:50,351 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-12 09:25:50,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-12 09:25:50,721 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-12 09:25:50,784 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 4417@H6
2019-09-12 09:25:50,786 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/home/luroz/hadoop/datanode/
java.io.IOException: Incompatible clusterIDs in /home/luroz/hadoop/datanode: namenode clusterID = CID-4195bb56-32a9-4757-8bb2-3655a6681aa1; datanode clusterID = CID-51d3ad8d-f2a6-4911-b240-848f0634fc7a
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:760)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:293)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:409)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:388)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:556)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 09:25:50,789 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shouldRetryInit(BPOfferService.java:826)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRetryInit(BPServiceActor.java:792)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:755)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 09:25:50,789 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:557)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 09:25:50,789 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956) service to localhost/127.0.0.1:9000
2019-09-12 09:25:50,890 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:90)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 09:25:50,891 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956)
2019-09-12 09:25:50,891 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1491)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 09:25:52,891 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-09-12 09:25:52,892 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-09-12 09:25:52,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-12 10:24:38,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-12 10:24:38,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-12 10:24:38,474 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 10:24:39,162 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-12 10:24:39,335 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-12 10:24:39,335 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-12 10:24:39,357 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-12 10:24:39,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-12 10:24:39,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-12 10:24:39,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-12 10:24:39,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-12 10:24:39,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-12 10:24:39,677 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-12 10:24:39,687 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-12 10:24:39,759 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-12 10:24:39,764 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-12 10:24:39,766 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-12 10:24:39,766 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-12 10:24:39,766 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-12 10:24:39,825 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45385
2019-09-12 10:24:39,825 INFO org.mortbay.log: jetty-6.1.26
2019-09-12 10:24:40,040 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45385
2019-09-12 10:24:40,355 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-12 10:24:40,359 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-12 10:24:40,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-12 10:24:40,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-12 10:24:40,657 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-12 10:24:40,670 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-12 10:24:40,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-12 10:24:40,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-12 10:24:40,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-12 10:24:40,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-12 10:24:40,933 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-12 10:24:40,933 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-12 10:24:41,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-12 10:24:41,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-12 10:24:41,330 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 10826@H6
2019-09-12 10:24:41,442 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/home/luroz/hadoop/datanode/
java.io.IOException: Incompatible clusterIDs in /home/luroz/hadoop/datanode: namenode clusterID = CID-4195bb56-32a9-4757-8bb2-3655a6681aa1; datanode clusterID = CID-51d3ad8d-f2a6-4911-b240-848f0634fc7a
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:760)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:293)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:409)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:388)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:556)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 10:24:41,494 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shouldRetryInit(BPOfferService.java:826)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRetryInit(BPServiceActor.java:792)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:755)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 10:24:41,495 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:557)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 10:24:41,495 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956) service to localhost/127.0.0.1:9000
2019-09-12 10:24:41,596 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:90)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 10:24:41,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid 7308bf1f-ab7e-44ae-beb0-e551d711e956)
2019-09-12 10:24:41,596 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1491)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-09-12 10:24:43,596 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-09-12 10:24:43,613 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-09-12 10:24:43,615 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-12 14:09:09,598 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-12 14:09:09,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-12 14:09:09,950 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-12 14:09:10,438 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-12 14:09:10,489 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-12 14:09:10,489 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-12 14:09:10,495 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-12 14:09:10,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-12 14:09:10,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-12 14:09:10,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-12 14:09:10,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-12 14:09:10,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-12 14:09:10,620 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-12 14:09:10,626 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-12 14:09:10,642 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-12 14:09:10,650 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-12 14:09:10,651 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-12 14:09:10,651 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-12 14:09:10,651 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-12 14:09:10,674 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35495
2019-09-12 14:09:10,674 INFO org.mortbay.log: jetty-6.1.26
2019-09-12 14:09:10,812 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35495
2019-09-12 14:09:11,167 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-12 14:09:11,169 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-12 14:09:11,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-12 14:09:11,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-12 14:09:11,262 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-12 14:09:11,274 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-12 14:09:11,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-12 14:09:11,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-12 14:09:11,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-12 14:09:11,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-12 14:09:11,416 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-12 14:09:11,416 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-12 14:09:11,766 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-12 14:09:11,773 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-12 14:09:11,846 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 17405@H6
2019-09-12 14:09:11,847 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/luroz/hadoop/datanode is not formatted for namespace 1290032588. Formatting...
2019-09-12 14:09:11,869 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77 for directory /home/luroz/hadoop/datanode
2019-09-12 14:09:12,034 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-942232487-127.0.1.1-1568235982744
2019-09-12 14:09:12,034 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744
2019-09-12 14:09:12,035 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744 is not formatted for BP-942232487-127.0.1.1-1568235982744. Formatting ...
2019-09-12 14:09:12,035 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-942232487-127.0.1.1-1568235982744 directory /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current
2019-09-12 14:09:12,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1290032588;bpid=BP-942232487-127.0.1.1-1568235982744;lv=-57;nsInfo=lv=-63;cid=CID-4195bb56-32a9-4757-8bb2-3655a6681aa1;nsid=1290032588;c=1568235982744;bpid=BP-942232487-127.0.1.1-1568235982744;dnuuid=null
2019-09-12 14:09:12,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID fba7e3f5-80b1-4355-b759-4c5759a3ce9f
2019-09-12 14:09:12,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77
2019-09-12 14:09:12,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-09-12 14:09:12,269 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-12 14:09:12,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-12 14:09:12,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-942232487-127.0.1.1-1568235982744
2019-09-12 14:09:12,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-12 14:09:12,332 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-942232487-127.0.1.1-1568235982744 on /home/luroz/hadoop/datanode/current: 56ms
2019-09-12 14:09:12,332 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-942232487-127.0.1.1-1568235982744: 57ms
2019-09-12 14:09:12,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-12 14:09:12,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current/replicas doesn't exist 
2019-09-12 14:09:12,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current: 0ms
2019-09-12 14:09:12,334 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-09-12 14:09:12,335 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode
2019-09-12 14:09:12,341 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 9/12/19 4:51 PM with interval of 21600000ms
2019-09-12 14:09:12,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-12 14:09:12,348 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77): finished scanning block pool BP-942232487-127.0.1.1-1568235982744
2019-09-12 14:09:12,379 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77): no suitable block pools found to scan.  Waiting 1814399956 ms.
2019-09-12 14:09:12,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-12 14:09:12,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-12 14:09:12,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe9714935ba6b7592,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 42 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-12 14:09:12,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-942232487-127.0.1.1-1568235982744
2019-09-12 15:15:03,881 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-942232487-127.0.1.1-1568235982744:blk_1073741825_1001 src: /127.0.0.1:58990 dest: /127.0.0.1:50010
2019-09-12 15:23:52,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:58990, dest: /127.0.0.1:50010, bytes: 7750450, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_638691732_28, offset: 0, srvID: fba7e3f5-80b1-4355-b759-4c5759a3ce9f, blockid: BP-942232487-127.0.1.1-1568235982744:blk_1073741825_1001, duration: 528808994220
2019-09-12 15:23:52,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-942232487-127.0.1.1-1568235982744:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-09-12 15:25:41,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-942232487-127.0.1.1-1568235982744:blk_1073741826_1002 src: /127.0.0.1:33008 dest: /127.0.0.1:50010
2019-09-12 15:28:03,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2019-09-12 15:28:03,616 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-942232487-127.0.1.1-1568235982744 blk_1073741825_1001 file /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current/finalized/subdir0/subdir0/blk_1073741825
2019-09-12 15:35:38,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:33008, dest: /127.0.0.1:50010, bytes: 8914051, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_620632290_28, offset: 0, srvID: fba7e3f5-80b1-4355-b759-4c5759a3ce9f, blockid: BP-942232487-127.0.1.1-1568235982744:blk_1073741826_1002, duration: 597071221227
2019-09-12 15:35:38,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-942232487-127.0.1.1-1568235982744:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2019-09-12 15:35:42,997 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-942232487-127.0.1.1-1568235982744:blk_1073741827_1003 src: /127.0.0.1:35312 dest: /127.0.0.1:50010
2019-09-12 15:45:40,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:35312, dest: /127.0.0.1:50010, bytes: 3512586, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_620632290_28, offset: 0, srvID: fba7e3f5-80b1-4355-b759-4c5759a3ce9f, blockid: BP-942232487-127.0.1.1-1568235982744:blk_1073741827_1003, duration: 597066682427
2019-09-12 15:45:40,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-942232487-127.0.1.1-1568235982744:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-09-12 16:51:38,429 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-942232487-127.0.1.1-1568235982744 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-09-12 18:27:56,389 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-12 18:27:56,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-13 10:45:40,203 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-13 10:45:40,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-13 10:45:40,570 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-13 10:45:40,999 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-13 10:45:41,085 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-13 10:45:41,086 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-13 10:45:41,093 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-13 10:45:41,094 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-13 10:45:41,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-13 10:45:41,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-13 10:45:41,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-13 10:45:41,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-13 10:45:41,355 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-13 10:45:41,362 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-13 10:45:41,374 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-13 10:45:41,379 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-13 10:45:41,381 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-13 10:45:41,381 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-13 10:45:41,381 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-13 10:45:41,397 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42579
2019-09-13 10:45:41,397 INFO org.mortbay.log: jetty-6.1.26
2019-09-13 10:45:41,532 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42579
2019-09-13 10:45:42,730 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-13 10:45:42,734 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-13 10:45:42,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-13 10:45:42,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-13 10:45:43,180 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-13 10:45:43,285 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-13 10:45:43,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-13 10:45:43,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-13 10:45:43,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-13 10:45:43,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-13 10:45:43,637 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-13 10:45:43,638 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-13 10:45:44,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-13 10:45:45,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-13 10:45:45,361 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-13 10:45:45,423 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 4216@H6
2019-09-13 10:45:45,575 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-942232487-127.0.1.1-1568235982744
2019-09-13 10:45:45,575 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744
2019-09-13 10:45:45,648 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1290032588;bpid=BP-942232487-127.0.1.1-1568235982744;lv=-57;nsInfo=lv=-63;cid=CID-4195bb56-32a9-4757-8bb2-3655a6681aa1;nsid=1290032588;c=1568235982744;bpid=BP-942232487-127.0.1.1-1568235982744;dnuuid=fba7e3f5-80b1-4355-b759-4c5759a3ce9f
2019-09-13 10:45:45,833 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77
2019-09-13 10:45:45,835 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-09-13 10:45:45,839 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-13 10:45:45,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-13 10:45:45,847 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-942232487-127.0.1.1-1568235982744
2019-09-13 10:45:45,849 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-13 10:45:45,947 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-942232487-127.0.1.1-1568235982744 on /home/luroz/hadoop/datanode/current: 98ms
2019-09-13 10:45:45,948 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-942232487-127.0.1.1-1568235982744: 100ms
2019-09-13 10:45:45,949 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-13 10:45:45,949 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current/replicas doesn't exist 
2019-09-13 10:45:45,953 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current: 4ms
2019-09-13 10:45:45,953 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2019-09-13 10:45:46,053 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77): no suitable block pools found to scan.  Waiting 1740206282 ms.
2019-09-13 10:45:46,056 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 9/13/19 11:45 AM with interval of 21600000ms
2019-09-13 10:45:46,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-13 10:45:46,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-13 10:45:46,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-13 10:45:46,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xf9ef6edc023e0b87,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 4 msec to generate and 48 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-13 10:45:46,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-942232487-127.0.1.1-1568235982744
2019-09-13 10:58:16,268 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-09-13 10:58:20,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-13 10:58:21,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-13 10:58:21,362 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-13 10:58:21,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-13 15:49:38,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-13 15:49:38,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-13 15:49:38,667 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-13 15:49:39,071 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-13 15:49:39,132 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-13 15:49:39,132 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-13 15:49:39,137 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-13 15:49:39,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-13 15:49:39,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-13 15:49:39,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-13 15:49:39,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-13 15:49:39,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-13 15:49:39,271 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-13 15:49:39,280 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-13 15:49:39,299 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-13 15:49:39,308 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-13 15:49:39,312 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-13 15:49:39,312 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-13 15:49:39,312 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-13 15:49:39,336 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39523
2019-09-13 15:49:39,336 INFO org.mortbay.log: jetty-6.1.26
2019-09-13 15:49:39,521 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39523
2019-09-13 15:49:39,882 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-13 15:49:39,886 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-13 15:49:39,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-13 15:49:39,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-13 15:49:39,980 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-13 15:49:39,992 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-13 15:49:40,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-13 15:49:40,063 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-13 15:49:40,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-13 15:49:40,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-13 15:49:40,139 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-13 15:49:40,140 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-13 15:49:40,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-13 15:49:40,518 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-13 15:49:40,587 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 29273@H6
2019-09-13 15:49:40,702 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-942232487-127.0.1.1-1568235982744
2019-09-13 15:49:40,702 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744
2019-09-13 15:49:40,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1290032588;bpid=BP-942232487-127.0.1.1-1568235982744;lv=-57;nsInfo=lv=-63;cid=CID-4195bb56-32a9-4757-8bb2-3655a6681aa1;nsid=1290032588;c=1568235982744;bpid=BP-942232487-127.0.1.1-1568235982744;dnuuid=fba7e3f5-80b1-4355-b759-4c5759a3ce9f
2019-09-13 15:49:40,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77
2019-09-13 15:49:40,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-09-13 15:49:40,880 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-13 15:49:40,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-13 15:49:40,887 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-942232487-127.0.1.1-1568235982744
2019-09-13 15:49:40,888 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-13 15:49:41,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-942232487-127.0.1.1-1568235982744 on /home/luroz/hadoop/datanode/current: 113ms
2019-09-13 15:49:41,001 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-942232487-127.0.1.1-1568235982744: 113ms
2019-09-13 15:49:41,003 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-13 15:49:41,003 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current/replicas doesn't exist 
2019-09-13 15:49:41,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current: 3ms
2019-09-13 15:49:41,008 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-09-13 15:49:41,088 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77): no suitable block pools found to scan.  Waiting 1721971247 ms.
2019-09-13 15:49:41,094 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 9/13/19 9:25 PM with interval of 21600000ms
2019-09-13 15:49:41,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-13 15:49:41,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-13 15:49:41,136 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-13 15:49:41,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x9a9e0112501455fd,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 4 msec to generate and 53 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-13 15:49:41,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-942232487-127.0.1.1-1568235982744
2019-09-13 15:58:26,170 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-09-13 15:58:29,018 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-13 15:58:29,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-13 16:05:15,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-
6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.
1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-
mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-13 16:05:15,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-13 16:05:15,477 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-13 16:05:15,905 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-13 16:05:15,971 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-13 16:05:15,971 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-13 16:05:15,977 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-13 16:05:15,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-13 16:05:15,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-13 16:05:16,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-13 16:05:16,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-13 16:05:16,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-13 16:05:16,119 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-13 16:05:16,126 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-13 16:05:16,139 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-13 16:05:16,144 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-13 16:05:16,146 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-13 16:05:16,146 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-13 16:05:16,146 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-13 16:05:16,162 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40739
2019-09-13 16:05:16,162 INFO org.mortbay.log: jetty-6.1.26
2019-09-13 16:05:16,340 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40739
2019-09-13 16:05:16,812 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-13 16:05:16,817 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-13 16:05:16,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-13 16:05:16,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-13 16:05:17,003 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-13 16:05:17,015 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-13 16:05:17,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-13 16:05:17,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-13 16:05:17,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-13 16:05:17,147 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-13 16:05:17,156 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-13 16:05:17,156 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-13 16:05:17,632 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-13 16:05:17,643 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-13 16:05:17,691 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 11968@H6
2019-09-13 16:05:17,843 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-942232487-127.0.1.1-1568235982744
2019-09-13 16:05:17,843 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744
2019-09-13 16:05:17,906 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1290032588;bpid=BP-942232487-127.0.1.1-1568235982744;lv=-57;nsInfo=lv=-63;cid=CID-4195bb56-32a9-4757-8bb2-3655a6681aa1;nsid=1290032588;c=1568235982744;bpid=BP-942232487-127.0.1.1-1568235982744;dnuuid=fba7e3f5-80b1-4355-b759-4c5759a3ce9f
2019-09-13 16:05:18,128 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77
2019-09-13 16:05:18,128 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-09-13 16:05:18,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-13 16:05:18,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-13 16:05:18,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-942232487-127.0.1.1-1568235982744
2019-09-13 16:05:18,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-13 16:05:18,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current: 12587008
2019-09-13 16:05:18,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-942232487-127.0.1.1-1568235982744 on /home/luroz/hadoop/datanode/current: 131ms
2019-09-13 16:05:18,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-942232487-127.0.1.1-1568235982744: 134ms
2019-09-13 16:05:18,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current...
2019-09-13 16:05:18,273 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-942232487-127.0.1.1-1568235982744/current/replicas doesn't exist 
2019-09-13 16:05:18,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-942232487-127.0.1.1-1568235982744 on volume /home/luroz/hadoop/datanode/current: 5ms
2019-09-13 16:05:18,277 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-09-13 16:05:18,373 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-d3790e4c-1da7-4bd1-b106-cc191cf2bc77): no suitable block pools found to scan.  Waiting 1721033963 ms.
2019-09-13 16:05:18,378 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 9/13/19 7:16 PM with interval of 21600000ms
2019-09-13 16:05:18,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-13 16:05:18,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-942232487-127.0.1.1-1568235982744 (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-13 16:05:18,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-13 16:05:18,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xcfa4c85ecd4b000c,  containing 1 storage report(s), of which we sent 1. The reports had 2 total blocks and used 1 RPC(s). This took 16 msec to generate and 66 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-13 16:05:18,790 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-942232487-127.0.1.1-1568235982744
2019-09-16 14:24:25,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-16 14:24:25,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-16 14:24:25,629 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-16 14:24:26,144 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-16 14:24:26,196 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-16 14:24:26,196 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-16 14:24:26,202 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-16 14:24:26,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-16 14:24:26,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-16 14:24:26,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-16 14:24:26,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-16 14:24:26,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-16 14:24:26,334 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-16 14:24:26,341 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-16 14:24:26,353 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-16 14:24:26,358 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-16 14:24:26,359 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-16 14:24:26,360 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-16 14:24:26,360 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-16 14:24:26,395 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42965
2019-09-16 14:24:26,395 INFO org.mortbay.log: jetty-6.1.26
2019-09-16 14:24:26,568 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42965
2019-09-16 14:24:27,339 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-16 14:24:27,342 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-16 14:24:27,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-16 14:24:27,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-16 14:24:27,450 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-16 14:24:27,461 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-16 14:24:27,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-16 14:24:27,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-16 14:24:27,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-16 14:24:27,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-16 14:24:27,582 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-16 14:24:27,583 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-16 14:24:27,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-16 14:24:27,977 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-16 14:24:28,055 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 9625@H6
2019-09-16 14:24:28,106 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/home/luroz/hadoop/datanode/
java.io.IOException: Incompatible clusterIDs in /home/luroz/hadoop/datanode: namenode clusterID = CID-17626bae-6437-41c4-86cc-0b77d175031b; datanode clusterID = CID-4195bb56-32a9-4757-8bb2-3655a6681aa1
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:760)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:293)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:409)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:388)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:556)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-16 14:24:28,111 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shouldRetryInit(BPOfferService.java:826)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRetryInit(BPServiceActor.java:792)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:755)
	at java.lang.Thread.run(Thread.java:748)
2019-09-16 14:24:28,112 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:557)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-09-16 14:24:28,112 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f) service to localhost/127.0.0.1:9000
2019-09-16 14:24:28,213 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:90)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-09-16 14:24:28,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid fba7e3f5-80b1-4355-b759-4c5759a3ce9f)
2019-09-16 14:24:28,214 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1491)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-09-16 14:24:30,214 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-09-16 14:24:30,216 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-09-16 14:24:30,218 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-16 14:26:52,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-16 14:26:52,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-16 14:26:53,271 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-16 14:26:53,547 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-16 14:26:53,597 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-16 14:26:53,597 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-16 14:26:53,602 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-16 14:26:53,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-16 14:26:53,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-16 14:26:53,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-16 14:26:53,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-16 14:26:53,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-16 14:26:53,687 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-16 14:26:53,694 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-16 14:26:53,706 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-16 14:26:53,711 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-16 14:26:53,712 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-16 14:26:53,712 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-16 14:26:53,713 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-16 14:26:53,724 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36877
2019-09-16 14:26:53,724 INFO org.mortbay.log: jetty-6.1.26
2019-09-16 14:26:53,831 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36877
2019-09-16 14:26:54,005 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-16 14:26:54,007 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-16 14:26:54,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-16 14:26:54,032 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-16 14:26:54,093 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-16 14:26:54,103 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-16 14:26:54,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-16 14:26:54,158 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-16 14:26:54,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-16 14:26:54,173 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-16 14:26:54,180 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-16 14:26:54,180 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-16 14:26:54,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-16 14:26:54,399 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-16 14:26:54,438 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 10993@H6
2019-09-16 14:26:54,439 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/luroz/hadoop/datanode is not formatted for namespace 1193881298. Formatting...
2019-09-16 14:26:54,440 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734 for directory /home/luroz/hadoop/datanode
2019-09-16 14:26:54,602 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-09-16 14:26:54,602 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-09-16 14:26:54,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761 is not formatted for BP-654726240-127.0.1.1-1568657882761. Formatting ...
2019-09-16 14:26:54,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-654726240-127.0.1.1-1568657882761 directory /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current
2019-09-16 14:26:54,747 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=null
2019-09-16 14:26:54,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID a54ff741-af55-4ac5-ac12-85fe462f5535
2019-09-16 14:26:54,997 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-09-16 14:26:54,998 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-09-16 14:26:55,003 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-16 14:26:55,009 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-16 14:26:55,009 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-09-16 14:26:55,018 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-09-16 14:26:55,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 36ms
2019-09-16 14:26:55,054 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 45ms
2019-09-16 14:26:55,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-09-16 14:26:55,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-09-16 14:26:55,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 1ms
2019-09-16 14:26:55,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-09-16 14:26:55,060 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode
2019-09-16 14:26:55,176 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): finished scanning block pool BP-654726240-127.0.1.1-1568657882761
2019-09-16 14:26:55,179 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 9/16/19 4:43 PM with interval of 21600000ms
2019-09-16 14:26:55,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-16 14:26:55,212 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 1814399847 ms.
2019-09-16 14:26:55,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-16 14:26:55,260 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-16 14:26:55,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe718ba3d759da37b,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 42 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-16 14:26:55,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-09-16 16:12:07,491 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-09-16 16:12:11,479 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-16 16:12:11,482 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-09-18 14:49:20,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-09-18 14:49:20,312 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-09-18 14:49:20,683 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-09-18 14:49:21,685 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-09-18 14:49:21,744 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-09-18 14:49:21,744 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-09-18 14:49:21,798 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-09-18 14:49:21,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-09-18 14:49:21,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-09-18 14:49:22,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-09-18 14:49:22,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-09-18 14:49:22,367 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-09-18 14:49:22,672 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-09-18 14:49:22,678 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-09-18 14:49:22,692 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-09-18 14:49:22,696 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-09-18 14:49:22,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-09-18 14:49:22,699 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-09-18 14:49:22,699 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-09-18 14:49:22,820 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37391
2019-09-18 14:49:22,820 INFO org.mortbay.log: jetty-6.1.26
2019-09-18 14:49:22,982 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37391
2019-09-18 14:49:23,695 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-09-18 14:49:23,702 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-09-18 14:49:23,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-09-18 14:49:23,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-09-18 14:49:24,123 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-09-18 14:49:24,182 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-09-18 14:49:24,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-09-18 14:49:24,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-09-18 14:49:24,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-09-18 14:49:24,736 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-09-18 14:49:24,751 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-09-18 14:49:24,758 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-09-18 14:49:26,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-09-18 14:49:26,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-09-18 14:49:26,736 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-09-18 14:49:26,784 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 17256@H6
2019-09-18 14:49:26,890 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-09-18 14:49:26,891 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-09-18 14:49:26,976 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=a54ff741-af55-4ac5-ac12-85fe462f5535
2019-09-18 14:49:27,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-09-18 14:49:27,045 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-09-18 14:49:27,049 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-09-18 14:49:27,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-09-18 14:49:27,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-09-18 14:49:27,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-09-18 14:49:27,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 89ms
2019-09-18 14:49:27,146 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 90ms
2019-09-18 14:49:27,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-09-18 14:49:27,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-09-18 14:49:27,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 0ms
2019-09-18 14:49:27,148 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-09-18 14:49:27,206 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 1640247853 ms.
2019-09-18 14:49:27,211 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 9/18/19 7:54 PM with interval of 21600000ms
2019-09-18 14:49:27,214 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-09-18 14:49:27,269 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-09-18 14:49:27,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-09-18 14:49:27,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x5d92ada633937549,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 2 msec to generate and 36 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-09-18 14:49:27,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-09-18 15:06:19,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741825_1001 src: /127.0.0.1:41818 dest: /127.0.0.1:50010
2019-09-18 15:06:19,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41818, dest: /127.0.0.1:50010, bytes: 78075, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741825_1001, duration: 786389439
2019-09-18 15:06:19,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-09-18 15:06:26,181 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741826_1002 src: /127.0.0.1:41894 dest: /127.0.0.1:50010
2019-09-18 15:06:27,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41894, dest: /127.0.0.1:50010, bytes: 74991, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741826_1002, duration: 1099402692
2019-09-18 15:06:27,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2019-09-18 15:06:35,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741827_1003 src: /127.0.0.1:41994 dest: /127.0.0.1:50010
2019-09-18 15:06:36,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41994, dest: /127.0.0.1:50010, bytes: 70017, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741827_1003, duration: 1115003864
2019-09-18 15:06:36,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-09-18 15:06:45,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741828_1004 src: /127.0.0.1:42090 dest: /127.0.0.1:50010
2019-09-18 15:06:46,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42090, dest: /127.0.0.1:50010, bytes: 68288, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741828_1004, duration: 1166047987
2019-09-18 15:06:46,862 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2019-09-18 15:06:54,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741829_1005 src: /127.0.0.1:42188 dest: /127.0.0.1:50010
2019-09-18 15:06:55,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42188, dest: /127.0.0.1:50010, bytes: 66949, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741829_1005, duration: 957499597
2019-09-18 15:06:55,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2019-09-18 15:07:03,027 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741830_1006 src: /127.0.0.1:42270 dest: /127.0.0.1:50010
2019-09-18 15:07:04,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42270, dest: /127.0.0.1:50010, bytes: 77269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741830_1006, duration: 1331820580
2019-09-18 15:07:04,361 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2019-09-18 15:07:11,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741831_1007 src: /127.0.0.1:42340 dest: /127.0.0.1:50010
2019-09-18 15:07:12,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42340, dest: /127.0.0.1:50010, bytes: 76105, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741831_1007, duration: 1121585360
2019-09-18 15:07:12,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2019-09-18 15:07:19,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741832_1008 src: /127.0.0.1:42418 dest: /127.0.0.1:50010
2019-09-18 15:07:20,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42418, dest: /127.0.0.1:50010, bytes: 68780, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741832_1008, duration: 1120253441
2019-09-18 15:07:20,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2019-09-18 15:07:26,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741833_1009 src: /127.0.0.1:42490 dest: /127.0.0.1:50010
2019-09-18 15:07:27,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42490, dest: /127.0.0.1:50010, bytes: 70039, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741833_1009, duration: 1065264466
2019-09-18 15:07:27,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2019-09-18 15:07:34,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741834_1010 src: /127.0.0.1:42578 dest: /127.0.0.1:50010
2019-09-18 15:07:36,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42578, dest: /127.0.0.1:50010, bytes: 72957, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741834_1010, duration: 2490299219
2019-09-18 15:07:36,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2019-09-18 15:07:43,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741835_1011 src: /127.0.0.1:42674 dest: /127.0.0.1:50010
2019-09-18 15:07:44,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42674, dest: /127.0.0.1:50010, bytes: 69733, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741835_1011, duration: 1032552658
2019-09-18 15:07:44,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-09-18 15:07:54,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741836_1012 src: /127.0.0.1:42788 dest: /127.0.0.1:50010
2019-09-18 15:07:55,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42788, dest: /127.0.0.1:50010, bytes: 65962, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741836_1012, duration: 1425149208
2019-09-18 15:07:55,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741836_1012, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:02,570 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741837_1013 src: /127.0.0.1:42874 dest: /127.0.0.1:50010
2019-09-18 15:08:02,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42874, dest: /127.0.0.1:50010, bytes: 33936, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741837_1013, duration: 233780745
2019-09-18 15:08:02,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741837_1013, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:10,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741838_1014 src: /127.0.0.1:42950 dest: /127.0.0.1:50010
2019-09-18 15:08:11,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42950, dest: /127.0.0.1:50010, bytes: 74687, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741838_1014, duration: 1099732119
2019-09-18 15:08:11,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741838_1014, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:18,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741839_1015 src: /127.0.0.1:43040 dest: /127.0.0.1:50010
2019-09-18 15:08:19,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43040, dest: /127.0.0.1:50010, bytes: 69646, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741839_1015, duration: 1490691945
2019-09-18 15:08:19,995 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741839_1015, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:27,560 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741840_1016 src: /127.0.0.1:43128 dest: /127.0.0.1:50010
2019-09-18 15:08:28,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43128, dest: /127.0.0.1:50010, bytes: 69046, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741840_1016, duration: 1010995852
2019-09-18 15:08:28,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741840_1016, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:35,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741841_1017 src: /127.0.0.1:43192 dest: /127.0.0.1:50010
2019-09-18 15:08:36,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43192, dest: /127.0.0.1:50010, bytes: 68553, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741841_1017, duration: 1691386266
2019-09-18 15:08:36,699 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741841_1017, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:43,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741842_1018 src: /127.0.0.1:43274 dest: /127.0.0.1:50010
2019-09-18 15:08:44,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43274, dest: /127.0.0.1:50010, bytes: 72332, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741842_1018, duration: 1290697204
2019-09-18 15:08:44,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741842_1018, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:50,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741843_1019 src: /127.0.0.1:43354 dest: /127.0.0.1:50010
2019-09-18 15:08:51,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43354, dest: /127.0.0.1:50010, bytes: 76595, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741843_1019, duration: 1043389382
2019-09-18 15:08:51,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741843_1019, type=LAST_IN_PIPELINE terminating
2019-09-18 15:08:59,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741844_1020 src: /127.0.0.1:43440 dest: /127.0.0.1:50010
2019-09-18 15:09:00,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43440, dest: /127.0.0.1:50010, bytes: 72585, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741844_1020, duration: 1199282362
2019-09-18 15:09:00,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741844_1020, type=LAST_IN_PIPELINE terminating
2019-09-18 15:09:07,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741845_1021 src: /127.0.0.1:43530 dest: /127.0.0.1:50010
2019-09-18 15:09:08,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43530, dest: /127.0.0.1:50010, bytes: 67145, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741845_1021, duration: 999308796
2019-09-18 15:09:08,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741845_1021, type=LAST_IN_PIPELINE terminating
2019-09-18 15:09:16,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741846_1022 src: /127.0.0.1:43630 dest: /127.0.0.1:50010
2019-09-18 15:09:17,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43630, dest: /127.0.0.1:50010, bytes: 66817, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741846_1022, duration: 1099547068
2019-09-18 15:09:17,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741846_1022, type=LAST_IN_PIPELINE terminating
2019-09-18 15:09:25,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741847_1023 src: /127.0.0.1:43716 dest: /127.0.0.1:50010
2019-09-18 15:09:26,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43716, dest: /127.0.0.1:50010, bytes: 70212, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741847_1023, duration: 1132344629
2019-09-18 15:09:26,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741847_1023, type=LAST_IN_PIPELINE terminating
2019-09-18 15:09:37,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741848_1024 src: /127.0.0.1:43842 dest: /127.0.0.1:50010
2019-09-18 15:09:38,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43842, dest: /127.0.0.1:50010, bytes: 78864, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741848_1024, duration: 907127615
2019-09-18 15:09:38,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741848_1024, type=LAST_IN_PIPELINE terminating
2019-09-18 15:09:43,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741849_1025 src: /127.0.0.1:43902 dest: /127.0.0.1:50010
2019-09-18 15:09:44,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43902, dest: /127.0.0.1:50010, bytes: 65227, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741849_1025, duration: 1007762307
2019-09-18 15:09:44,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741849_1025, type=LAST_IN_PIPELINE terminating
2019-09-18 15:09:52,478 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741850_1026 src: /127.0.0.1:43992 dest: /127.0.0.1:50010
2019-09-18 15:09:53,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43992, dest: /127.0.0.1:50010, bytes: 67000, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741850_1026, duration: 1244522750
2019-09-18 15:09:53,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741850_1026, type=LAST_IN_PIPELINE terminating
2019-09-18 15:10:00,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741851_1027 src: /127.0.0.1:44062 dest: /127.0.0.1:50010
2019-09-18 15:10:02,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44062, dest: /127.0.0.1:50010, bytes: 31766, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741851_1027, duration: 2180565605
2019-09-18 15:10:02,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741851_1027, type=LAST_IN_PIPELINE terminating
2019-09-18 15:10:08,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741852_1028 src: /127.0.0.1:44154 dest: /127.0.0.1:50010
2019-09-18 15:10:09,670 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44154, dest: /127.0.0.1:50010, bytes: 89004, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741852_1028, duration: 1016305129
2019-09-18 15:10:09,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741852_1028, type=LAST_IN_PIPELINE terminating
2019-09-18 15:10:16,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741853_1029 src: /127.0.0.1:44234 dest: /127.0.0.1:50010
2019-09-18 15:10:17,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44234, dest: /127.0.0.1:50010, bytes: 70016, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741853_1029, duration: 1074354820
2019-09-18 15:10:17,870 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741853_1029, type=LAST_IN_PIPELINE terminating
2019-09-18 15:10:25,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741854_1030 src: /127.0.0.1:44318 dest: /127.0.0.1:50010
2019-09-18 15:10:26,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44318, dest: /127.0.0.1:50010, bytes: 66406, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741854_1030, duration: 1090781011
2019-09-18 15:10:26,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741854_1030, type=LAST_IN_PIPELINE terminating
2019-09-18 15:10:34,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741855_1031 src: /127.0.0.1:44408 dest: /127.0.0.1:50010
2019-09-18 15:16:02,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44408, dest: /127.0.0.1:50010, bytes: 3212532, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1270072765_28, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741855_1031, duration: 328296528682
2019-09-18 15:16:02,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741855_1031, type=LAST_IN_PIPELINE terminating
2019-09-18 15:18:23,873 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-09-18 15:18:24,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-02 07:54:53,105 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-02 07:54:53,130 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-02 07:54:53,492 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-02 07:54:54,434 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-02 07:54:54,485 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-02 07:54:54,485 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-02 07:54:54,499 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-02 07:54:54,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-02 07:54:54,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-02 07:54:54,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-02 07:54:54,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-02 07:54:54,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-02 07:54:54,896 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-02 07:54:54,903 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-02 07:54:54,920 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-02 07:54:54,925 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-02 07:54:54,926 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-02 07:54:54,926 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-02 07:54:54,926 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-02 07:54:55,034 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46337
2019-10-02 07:54:55,034 INFO org.mortbay.log: jetty-6.1.26
2019-10-02 07:54:55,169 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46337
2019-10-02 07:54:57,267 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-02 07:54:57,269 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-02 07:54:57,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-02 07:54:57,308 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-02 07:54:58,004 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-02 07:54:58,092 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-02 07:54:58,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-02 07:54:58,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-02 07:54:58,724 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-02 07:54:58,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-02 07:54:58,978 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-02 07:54:58,979 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-02 07:55:00,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-02 07:55:00,873 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-02 07:55:00,890 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-02 07:55:01,030 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 6329@H6
2019-10-02 07:55:01,204 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-10-02 07:55:01,204 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-10-02 07:55:01,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=a54ff741-af55-4ac5-ac12-85fe462f5535
2019-10-02 07:55:01,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-10-02 07:55:01,437 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-02 07:55:01,440 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-02 07:55:01,445 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-02 07:55:01,445 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-10-02 07:55:01,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-02 07:55:01,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 189ms
2019-10-02 07:55:01,635 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 190ms
2019-10-02 07:55:01,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-02 07:55:01,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-10-02 07:55:01,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 10ms
2019-10-02 07:55:01,646 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 10ms
2019-10-02 07:55:01,772 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 455513287 ms.
2019-10-02 07:55:01,778 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/2/19 1:00 PM with interval of 21600000ms
2019-10-02 07:55:01,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-02 07:55:01,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-02 07:55:01,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-02 07:55:02,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6bf36aecfe977bde,  containing 1 storage report(s), of which we sent 1. The reports had 31 total blocks and used 1 RPC(s). This took 3 msec to generate and 73 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-02 07:55:02,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-02 09:27:07,294 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-02 09:27:11,269 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-02 09:27:11,745 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-02 09:27:11,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-03 10:10:15,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-03 10:10:15,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-03 10:10:15,780 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-03 10:10:21,731 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-03 10:10:23,041 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-03 10:10:23,041 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-03 10:10:23,109 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-03 10:10:23,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-03 10:10:23,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-03 10:10:23,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-03 10:10:23,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-03 10:10:23,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-03 10:10:23,673 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-03 10:10:23,679 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-03 10:10:23,692 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-03 10:10:23,696 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-03 10:10:23,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-03 10:10:23,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-03 10:10:23,698 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-03 10:10:23,939 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36287
2019-10-03 10:10:23,939 INFO org.mortbay.log: jetty-6.1.26
2019-10-03 10:10:24,217 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36287
2019-10-03 10:10:26,486 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-03 10:10:26,488 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-03 10:10:26,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-03 10:10:26,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-03 10:10:27,383 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-03 10:10:27,698 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-03 10:10:28,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-03 10:10:28,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-03 10:10:29,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-03 10:10:29,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-03 10:10:29,301 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-03 10:10:29,302 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-03 10:10:31,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-03 10:10:32,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-03 10:10:33,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-03 10:10:33,173 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-03 10:10:33,331 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 9327@H6
2019-10-03 10:10:34,324 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-10-03 10:10:34,324 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-10-03 10:10:34,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=a54ff741-af55-4ac5-ac12-85fe462f5535
2019-10-03 10:10:35,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-10-03 10:10:35,394 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-03 10:10:35,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-03 10:10:35,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-03 10:10:35,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-10-03 10:10:35,415 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-03 10:10:36,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 935ms
2019-10-03 10:10:36,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 939ms
2019-10-03 10:10:36,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-03 10:10:36,353 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-10-03 10:10:36,379 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 27ms
2019-10-03 10:10:36,379 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 27ms
2019-10-03 10:10:36,884 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 360978175 ms.
2019-10-03 10:10:36,994 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/3/19 4:00 PM with interval of 21600000ms
2019-10-03 10:10:36,996 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-03 10:10:38,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-03 10:10:38,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-03 10:10:40,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xa72ebe9b9f8613dd,  containing 1 storage report(s), of which we sent 1. The reports had 31 total blocks and used 1 RPC(s). This took 54 msec to generate and 287 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-03 10:10:40,915 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-03 11:06:00,377 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-03 11:06:00,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-03 11:06:00,780 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-03 11:06:02,238 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-03 11:06:02,297 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-03 11:06:02,297 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-03 11:06:02,321 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-03 11:06:02,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-03 11:06:02,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-03 11:06:02,491 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-03 11:06:02,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-03 11:06:02,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-03 11:06:02,702 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-03 11:06:02,715 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-03 11:06:02,735 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-03 11:06:02,742 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-03 11:06:02,744 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-03 11:06:02,744 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-03 11:06:02,744 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-03 11:06:02,808 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35607
2019-10-03 11:06:02,808 INFO org.mortbay.log: jetty-6.1.26
2019-10-03 11:06:02,961 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35607
2019-10-03 11:06:03,884 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-03 11:06:03,886 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-03 11:06:03,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-03 11:06:03,932 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-03 11:06:04,221 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-03 11:06:04,242 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-03 11:06:04,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-03 11:06:04,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-03 11:06:04,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-03 11:06:04,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-03 11:06:04,744 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-03 11:06:04,744 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-03 11:06:05,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-03 11:06:05,732 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-03 11:06:05,801 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 4587@H6
2019-10-03 11:06:06,074 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-10-03 11:06:06,075 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-10-03 11:06:06,276 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=a54ff741-af55-4ac5-ac12-85fe462f5535
2019-10-03 11:06:06,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-10-03 11:06:06,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-03 11:06:06,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-03 11:06:06,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-03 11:06:06,612 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-10-03 11:06:06,653 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-03 11:06:06,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 143ms
2019-10-03 11:06:06,796 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 184ms
2019-10-03 11:06:06,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-03 11:06:06,799 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-10-03 11:06:06,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 10ms
2019-10-03 11:06:06,810 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2019-10-03 11:06:06,973 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 357648086 ms.
2019-10-03 11:06:06,979 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/3/19 12:03 PM with interval of 21600000ms
2019-10-03 11:06:06,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-03 11:06:07,046 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-03 11:06:07,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-03 11:06:07,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x745c6a9087dcfb3a,  containing 1 storage report(s), of which we sent 1. The reports had 31 total blocks and used 1 RPC(s). This took 4 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-03 11:06:07,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-03 12:03:17,200 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-654726240-127.0.1.1-1568657882761 Total blocks: 31, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-10-04 09:36:50,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-04 09:36:50,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-04 09:36:50,821 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-04 09:36:51,772 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-04 09:36:51,823 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-04 09:36:51,823 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-04 09:36:51,865 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-04 09:36:51,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-04 09:36:52,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-04 09:36:52,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-04 09:36:52,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-04 09:36:52,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-04 09:36:52,614 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-04 09:36:52,621 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-04 09:36:52,634 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-04 09:36:52,639 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-04 09:36:52,640 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-04 09:36:52,640 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-04 09:36:52,641 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-04 09:36:52,737 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39195
2019-10-04 09:36:52,737 INFO org.mortbay.log: jetty-6.1.26
2019-10-04 09:36:52,861 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39195
2019-10-04 09:36:55,106 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-04 09:36:55,128 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-04 09:36:55,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-04 09:36:55,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-04 09:36:56,359 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-04 09:36:56,765 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-04 09:36:57,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-04 09:36:57,684 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-04 09:36:57,691 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-04 09:36:57,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-04 09:36:57,721 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-04 09:36:57,722 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-04 09:37:00,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-04 09:37:01,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-04 09:37:01,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-04 09:37:01,687 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-04 09:37:01,907 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 5928@H6
2019-10-04 09:37:02,249 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-10-04 09:37:02,249 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-10-04 09:37:02,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=a54ff741-af55-4ac5-ac12-85fe462f5535
2019-10-04 09:37:02,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-10-04 09:37:02,598 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-04 09:37:02,602 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-04 09:37:02,622 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-04 09:37:02,624 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-10-04 09:37:02,625 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-04 09:37:02,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 207ms
2019-10-04 09:37:02,832 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 208ms
2019-10-04 09:37:02,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-04 09:37:02,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-10-04 09:37:02,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 15ms
2019-10-04 09:37:02,859 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 25ms
2019-10-04 09:37:03,027 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 276592032 ms.
2019-10-04 09:37:03,033 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/4/19 3:15 PM with interval of 21600000ms
2019-10-04 09:37:03,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-04 09:37:03,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-04 09:37:03,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-04 09:37:03,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xecc61084803a4e1c,  containing 1 storage report(s), of which we sent 1. The reports had 31 total blocks and used 1 RPC(s). This took 4 msec to generate and 92 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-04 09:37:03,675 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-04 09:56:02,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xecc61084803a4e1d,  containing 1 storage report(s), of which we sent 1. The reports had 31 total blocks and used 1 RPC(s). This took 1 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-04 09:56:02,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-04 15:08:10,591 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-04 15:08:10,599 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-09 10:00:53,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-09 10:00:53,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-09 10:00:53,422 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-09 10:00:54,006 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-09 10:00:54,061 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-09 10:00:54,061 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-09 10:00:54,088 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-09 10:00:54,149 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-09 10:00:54,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-09 10:00:54,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-09 10:00:54,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-09 10:00:54,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-09 10:00:54,726 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-09 10:00:54,732 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-09 10:00:54,745 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-09 10:00:54,750 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-09 10:00:54,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-09 10:00:54,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-09 10:00:54,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-09 10:00:54,908 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 32875
2019-10-09 10:00:54,908 INFO org.mortbay.log: jetty-6.1.26
2019-10-09 10:00:55,108 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:32875
2019-10-09 10:00:56,598 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-09 10:00:56,621 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-09 10:00:56,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-09 10:00:56,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-09 10:00:57,499 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-09 10:00:57,607 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-09 10:00:58,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-09 10:00:58,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-09 10:00:58,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-09 10:00:58,427 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-09 10:00:58,552 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-09 10:00:58,552 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-09 10:01:00,436 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-09 10:01:01,437 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-09 10:01:02,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-09 10:01:03,438 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-09 10:01:04,439 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-09 10:01:05,440 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-09 10:01:06,441 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-09 10:01:06,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-09 10:01:06,639 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-09 10:01:06,827 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 6887@H6
2019-10-09 10:01:07,001 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-10-09 10:01:07,002 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-10-09 10:01:07,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=a54ff741-af55-4ac5-ac12-85fe462f5535
2019-10-09 10:01:07,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-10-09 10:01:07,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-09 10:01:07,330 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-09 10:01:07,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-09 10:01:07,337 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-10-09 10:01:07,338 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-09 10:01:07,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 222ms
2019-10-09 10:01:07,561 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 224ms
2019-10-09 10:01:07,565 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-09 10:01:07,565 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-10-09 10:01:07,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 11ms
2019-10-09 10:01:07,575 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 11ms
2019-10-09 10:01:07,652 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode, after more than 504 hour(s)
2019-10-09 10:01:07,657 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/9/19 4:00 PM with interval of 21600000ms
2019-10-09 10:01:07,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-09 10:01:07,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-09 10:01:07,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-09 10:01:07,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4860ef79b3fd67c2,  containing 1 storage report(s), of which we sent 1. The reports had 31 total blocks and used 1 RPC(s). This took 4 msec to generate and 50 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-09 10:01:07,990 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-09 10:02:12,426 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): finished scanning block pool BP-654726240-127.0.1.1-1568657882761
2019-10-09 10:02:12,458 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 1814335194 ms.
2019-10-09 10:57:37,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741856_1032 src: /127.0.0.1:36296 dest: /127.0.0.1:50010
2019-10-09 10:57:37,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:36296, dest: /127.0.0.1:50010, bytes: 28094, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1501368999_1, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741856_1032, duration: 7324468
2019-10-09 10:57:37,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741856_1032, type=LAST_IN_PIPELINE terminating
2019-10-09 11:02:30,997 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1610ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1024ms
2019-10-09 11:40:44,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4860ef79b3fd67c3,  containing 1 storage report(s), of which we sent 1. The reports had 32 total blocks and used 1 RPC(s). This took 0 msec to generate and 15 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-09 11:40:44,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-09 16:00:42,572 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1081ms
No GCs detected
2019-10-09 16:00:48,835 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-654726240-127.0.1.1-1568657882761 Total blocks: 32, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-10-09 16:00:49,687 WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Lock held time above threshold: lock identifier: org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl lockHeldTimeMs=359 ms. Suppressed 0 lock warnings. The stack trace is: java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1033)
org.apache.hadoop.hdfs.InstrumentedLock.logWarning(InstrumentedLock.java:145)
org.apache.hadoop.hdfs.InstrumentedLock.check(InstrumentedLock.java:181)
org.apache.hadoop.hdfs.InstrumentedLock.unlock(InstrumentedLock.java:135)
org.apache.hadoop.util.AutoCloseableLock.release(AutoCloseableLock.java:84)
org.apache.hadoop.util.AutoCloseableLock.close(AutoCloseableLock.java:96)
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.scan(DirectoryScanner.java:669)
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:569)
org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:514)
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
java.lang.Thread.run(Thread.java:748)

2019-10-09 16:11:44,006 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741857_1033 src: /127.0.0.1:41088 dest: /127.0.0.1:50010
2019-10-09 16:11:45,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41088, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_398892915_1, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741857_1033, duration: 423345069
2019-10-09 16:11:45,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741857_1033, type=LAST_IN_PIPELINE terminating
2019-10-09 16:11:49,789 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741858_1034 src: /127.0.0.1:41090 dest: /127.0.0.1:50010
2019-10-09 16:11:49,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41090, dest: /127.0.0.1:50010, bytes: 3734, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_398892915_1, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741858_1034, duration: 27993846
2019-10-09 16:11:49,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741858_1034, type=LAST_IN_PIPELINE terminating
2019-10-09 16:11:50,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741859_1035 src: /127.0.0.1:41092 dest: /127.0.0.1:50010
2019-10-09 16:11:51,126 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41092, dest: /127.0.0.1:50010, bytes: 348, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_398892915_1, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741859_1035, duration: 261044104
2019-10-09 16:11:51,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741859_1035, type=LAST_IN_PIPELINE terminating
2019-10-09 16:11:52,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741860_1036 src: /127.0.0.1:41094 dest: /127.0.0.1:50010
2019-10-09 16:11:53,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41094, dest: /127.0.0.1:50010, bytes: 135150, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_398892915_1, offset: 0, srvID: a54ff741-af55-4ac5-ac12-85fe462f5535, blockid: BP-654726240-127.0.1.1-1568657882761:blk_1073741860_1036, duration: 271180062
2019-10-09 16:11:53,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741860_1036, type=LAST_IN_PIPELINE terminating
2019-10-09 17:40:44,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4860ef79b3fd67c4,  containing 1 storage report(s), of which we sent 1. The reports had 36 total blocks and used 1 RPC(s). This took 31 msec to generate and 207 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-09 17:40:44,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-09 17:43:07,135 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1573ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1893ms
2019-10-09 22:00:38,716 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-654726240-127.0.1.1-1568657882761 Total blocks: 36, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-10-09 23:40:44,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4860ef79b3fd67c5,  containing 1 storage report(s), of which we sent 1. The reports had 36 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-09 23:40:44,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-10 08:57:17,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 08:57:17,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 08:57:17,779 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 08:57:18,512 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 08:57:18,595 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 08:57:18,595 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 08:57:18,618 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 08:57:18,671 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 08:57:18,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 08:57:18,839 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 08:57:18,885 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 08:57:18,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 08:57:19,289 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 08:57:19,295 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 08:57:19,308 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 08:57:19,315 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 08:57:19,317 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 08:57:19,317 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 08:57:19,317 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 08:57:19,452 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 41889
2019-10-10 08:57:19,452 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 08:57:19,620 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:41889
2019-10-10 08:57:22,031 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 08:57:22,048 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 08:57:22,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 08:57:22,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 08:57:23,086 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 08:57:23,231 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 08:57:23,929 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 08:57:23,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 08:57:23,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 08:57:24,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 08:57:24,240 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 08:57:24,243 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 08:57:25,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:26,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:27,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:28,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:29,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:30,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:31,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:32,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:33,981 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:34,982 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 08:57:35,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 08:57:35,311 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 08:57:35,395 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 4183@H6
2019-10-10 08:57:35,535 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-654726240-127.0.1.1-1568657882761
2019-10-10 08:57:35,535 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761
2019-10-10 08:57:35,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1193881298;bpid=BP-654726240-127.0.1.1-1568657882761;lv=-57;nsInfo=lv=-63;cid=CID-17626bae-6437-41c4-86cc-0b77d175031b;nsid=1193881298;c=1568657882761;bpid=BP-654726240-127.0.1.1-1568657882761;dnuuid=a54ff741-af55-4ac5-ac12-85fe462f5535
2019-10-10 08:57:35,708 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734
2019-10-10 08:57:35,708 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-10 08:57:35,714 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 08:57:35,722 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 08:57:35,722 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-654726240-127.0.1.1-1568657882761
2019-10-10 08:57:35,723 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-10 08:57:35,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-654726240-127.0.1.1-1568657882761 on /home/luroz/hadoop/datanode/current: 149ms
2019-10-10 08:57:35,873 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-654726240-127.0.1.1-1568657882761: 151ms
2019-10-10 08:57:35,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current...
2019-10-10 08:57:35,875 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/replicas doesn't exist 
2019-10-10 08:57:35,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-654726240-127.0.1.1-1568657882761 on volume /home/luroz/hadoop/datanode/current: 11ms
2019-10-10 08:57:35,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2019-10-10 08:57:35,955 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/datanode, DS-3656b3b0-39f8-4e37-8881-aca5d8d9e734): no suitable block pools found to scan.  Waiting 1731811697 ms.
2019-10-10 08:57:35,962 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 9:47 AM with interval of 21600000ms
2019-10-10 08:57:35,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 08:57:36,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-654726240-127.0.1.1-1568657882761 (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 08:57:36,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 08:57:36,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc2e9225210f3b7a1,  containing 1 storage report(s), of which we sent 1. The reports had 36 total blocks and used 1 RPC(s). This took 4 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 08:57:36,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-654726240-127.0.1.1-1568657882761
2019-10-10 09:25:05,413 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1111ms
No GCs detected
2019-10-10 09:25:46,492 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 37835ms
GC pool 'PS Scavenge' had collection(s): count=1 time=37434ms
2019-10-10 09:26:07,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-654726240-127.0.1.1-1568657882761:blk_1073741861_1037 src: /127.0.0.1:58332 dest: /127.0.0.1:50010
2019-10-10 09:26:11,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-654726240-127.0.1.1-1568657882761:blk_1073741861_1037
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:208)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:521)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:923)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:854)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:166)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:103)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:26:12,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741861_1037, type=LAST_IN_PIPELINE: Thread is interrupted.
2019-10-10 09:26:12,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-654726240-127.0.1.1-1568657882761:blk_1073741861_1037, type=LAST_IN_PIPELINE terminating
2019-10-10 09:26:12,922 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-654726240-127.0.1.1-1568657882761:blk_1073741861_1037 received exception java.io.IOException: Premature EOF from inputStream
2019-10-10 09:26:14,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: H6:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:58332 dst: /127.0.0.1:50010
java.io.IOException: Premature EOF from inputStream
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:208)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:211)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:521)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:923)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:854)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:166)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:103)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:26:19,631 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/rbw/blk_1073741861 for deletion
2019-10-10 09:26:19,820 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-654726240-127.0.1.1-1568657882761 blk_1073741861_1037 file /home/luroz/hadoop/datanode/current/BP-654726240-127.0.1.1-1568657882761/current/rbw/blk_1073741861
2019-10-10 09:34:57,906 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 09:35:00,984 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 09:35:01,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 09:37:10,337 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 09:37:10,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 09:37:10,651 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 09:37:11,220 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 09:37:11,291 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 09:37:11,291 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 09:37:11,405 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 09:37:11,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 09:37:11,409 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 09:37:11,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 09:37:11,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 09:37:11,537 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 09:37:11,885 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 09:37:11,896 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 09:37:11,923 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 09:37:11,928 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 09:37:11,930 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 09:37:11,930 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 09:37:11,930 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 09:37:12,037 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34247
2019-10-10 09:37:12,037 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 09:37:12,307 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34247
2019-10-10 09:37:16,323 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 09:37:16,326 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 09:37:16,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 09:37:16,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 09:37:16,411 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 09:37:16,422 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 09:37:16,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 09:37:16,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 09:37:16,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 09:37:16,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 09:37:16,596 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 09:37:16,597 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 09:37:17,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 09:37:17,776 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 09:37:18,082 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 11200@H6
2019-10-10 09:37:18,225 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/home/luroz/hadoop/datanode/
java.io.IOException: Incompatible clusterIDs in /home/luroz/hadoop/datanode: namenode clusterID = CID-eb21b4f1-ae77-4a5b-858b-a94688336e9a; datanode clusterID = CID-17626bae-6437-41c4-86cc-0b77d175031b
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:760)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:293)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:409)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:388)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:556)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:37:18,230 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shouldRetryInit(BPOfferService.java:826)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRetryInit(BPServiceActor.java:792)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:755)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:37:18,230 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:557)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:37:18,231 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000
2019-10-10 09:37:18,332 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:90)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:37:18,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535)
2019-10-10 09:37:18,333 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1491)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:37:20,334 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-10-10 09:37:20,335 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-10-10 09:37:20,340 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 09:38:06,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 09:38:06,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 09:38:07,211 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 09:38:07,475 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 09:38:07,527 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 09:38:07,527 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 09:38:07,532 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 09:38:07,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 09:38:07,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 09:38:07,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 09:38:07,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 09:38:07,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 09:38:07,611 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 09:38:07,617 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 09:38:07,630 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 09:38:07,635 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 09:38:07,636 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 09:38:07,636 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 09:38:07,636 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 09:38:07,649 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43015
2019-10-10 09:38:07,649 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 09:38:07,762 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43015
2019-10-10 09:38:07,903 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 09:38:07,906 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 09:38:07,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 09:38:07,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 09:38:07,996 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 09:38:08,007 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 09:38:08,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 09:38:08,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 09:38:08,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 09:38:08,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 09:38:08,087 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 09:38:08,090 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 09:38:08,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 09:38:08,176 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 09:38:08,315 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 12322@H6
2019-10-10 09:38:08,318 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/home/luroz/hadoop/datanode/
java.io.IOException: Incompatible clusterIDs in /home/luroz/hadoop/datanode: namenode clusterID = CID-eb21b4f1-ae77-4a5b-858b-a94688336e9a; datanode clusterID = CID-17626bae-6437-41c4-86cc-0b77d175031b
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:760)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:293)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:409)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:388)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:556)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:38:08,323 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shouldRetryInit(BPOfferService.java:826)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRetryInit(BPServiceActor.java:792)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:755)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:38:08,323 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:557)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:38:08,324 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000
2019-10-10 09:38:08,424 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:90)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:38:08,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535)
2019-10-10 09:38:08,425 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1491)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:38:10,426 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-10-10 09:38:10,427 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-10-10 09:38:10,429 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 09:51:19,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 09:51:19,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 09:51:19,628 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 09:51:20,016 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 09:51:20,073 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 09:51:20,073 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 09:51:20,082 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 09:51:20,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 09:51:20,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 09:51:20,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 09:51:20,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 09:51:20,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 09:51:20,210 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 09:51:20,217 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 09:51:20,230 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 09:51:20,235 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 09:51:20,236 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 09:51:20,237 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 09:51:20,237 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 09:51:20,267 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 36579
2019-10-10 09:51:20,267 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 09:51:20,389 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36579
2019-10-10 09:51:21,055 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 09:51:21,058 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 09:51:21,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 09:51:21,084 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 09:51:21,145 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 09:51:21,155 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 09:51:21,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 09:51:21,237 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 09:51:21,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 09:51:21,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 09:51:21,346 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 09:51:21,346 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 09:51:21,755 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 09:51:21,824 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 09:51:21,871 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/datanode/in_use.lock acquired by nodename 4687@H6
2019-10-10 09:51:21,896 WARN org.apache.hadoop.hdfs.server.common.Storage: Failed to add storage directory [DISK]file:/home/luroz/hadoop/datanode/
java.io.IOException: Incompatible clusterIDs in /home/luroz/hadoop/datanode: namenode clusterID = CID-eb21b4f1-ae77-4a5b-858b-a94688336e9a; datanode clusterID = CID-17626bae-6437-41c4-86cc-0b77d175031b
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:760)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:293)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:409)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:388)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:556)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:51:21,899 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shouldRetryInit(BPOfferService.java:826)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.shouldRetryInit(BPServiceActor.java:792)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:755)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:51:21,899 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000. Exiting. 
java.io.IOException: All specified directories are failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:557)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1574)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1535)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:382)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:266)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:51:21,899 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535) service to localhost/127.0.0.1:9000
2019-10-10 09:51:22,000 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:90)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1490)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:51:22,000 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid a54ff741-af55-4ac5-ac12-85fe462f5535)
2019-10-10 09:51:22,000 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN, trace:
java.lang.Exception
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:210)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.hasBlockPoolId(BPOfferService.java:220)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:1491)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:465)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:527)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:787)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 09:51:24,001 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2019-10-10 09:51:24,002 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2019-10-10 09:51:24,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 10:13:49,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 10:13:49,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 10:13:50,230 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 10:13:50,556 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 10:13:50,607 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 10:13:50,607 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 10:13:50,613 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 10:13:50,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 10:13:50,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 10:13:50,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 10:13:50,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 10:13:50,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 10:13:50,694 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 10:13:50,701 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 10:13:50,716 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 10:13:50,721 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 10:13:50,723 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 10:13:50,723 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 10:13:50,723 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 10:13:50,736 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42513
2019-10-10 10:13:50,736 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 10:13:50,851 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42513
2019-10-10 10:13:51,056 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 10:13:51,058 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 10:13:51,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 10:13:51,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 10:13:51,142 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 10:13:51,152 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 10:13:51,199 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 10:13:51,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 10:13:51,215 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 10:13:51,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 10:13:51,240 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 10:13:51,242 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 10:13:51,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 10:13:51,430 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 10:13:51,471 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 7008@H6
2019-10-10 10:13:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/luroz/hadoop/store/datanode is not formatted for namespace 1698661175. Formatting...
2019-10-10 10:13:51,473 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-ef6c53c4-807b-4700-bec3-0a3d36e71023 for directory /home/luroz/hadoop/store/datanode
2019-10-10 10:13:51,595 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 10:13:51,595 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 10:13:51,595 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840 is not formatted for BP-1876871648-127.0.1.1-1570716796840. Formatting ...
2019-10-10 10:13:51,596 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1876871648-127.0.1.1-1570716796840 directory /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current
2019-10-10 10:13:51,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=null
2019-10-10 10:13:51,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 43215549-03ef-4122-81db-88cb805594c7
2019-10-10 10:13:51,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 10:13:51,824 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 10:13:51,828 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 10:13:51,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 10:13:51,834 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 10:13:51,835 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 10:13:51,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 27ms
2019-10-10 10:13:51,862 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 28ms
2019-10-10 10:13:51,864 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 10:13:51,864 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 10:13:51,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 0ms
2019-10-10 10:13:51,865 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-10-10 10:13:51,866 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode
2019-10-10 10:13:51,873 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 11:30 AM with interval of 21600000ms
2019-10-10 10:13:51,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 10:13:51,878 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): finished scanning block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 10:13:51,914 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1814399952 ms.
2019-10-10 10:13:51,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 10:13:51,924 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 10:13:52,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6a8897cd017894d1,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 10:13:52,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 10:17:55,103 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741825_1001 src: /127.0.0.1:40958 dest: /127.0.0.1:50010
2019-10-10 10:17:55,183 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40958, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1057881072_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741825_1001, duration: 11684849
2019-10-10 10:17:55,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-10-10 10:17:57,956 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741825 for deletion
2019-10-10 10:17:57,959 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741825_1001 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741825
2019-10-10 10:25:29,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741826_1002 src: /127.0.0.1:41146 dest: /127.0.0.1:50010
2019-10-10 10:25:29,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41146, dest: /127.0.0.1:50010, bytes: 28094, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-107391080_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741826_1002, duration: 11667608
2019-10-10 10:25:29,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2019-10-10 10:48:41,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741827_1003 src: /127.0.0.1:41862 dest: /127.0.0.1:50010
2019-10-10 10:49:03,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41862, dest: /127.0.0.1:50010, bytes: 2890, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741827_1003, duration: 22326862035
2019-10-10 10:49:03,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-10-10 10:49:06,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741828_1004 src: /127.0.0.1:41870 dest: /127.0.0.1:50010
2019-10-10 10:49:07,433 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41870, dest: /127.0.0.1:50010, bytes: 85788, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741828_1004, duration: 1147978586
2019-10-10 10:49:07,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2019-10-10 10:49:08,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741829_1005 src: /127.0.0.1:41872 dest: /127.0.0.1:50010
2019-10-10 10:49:09,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41872, dest: /127.0.0.1:50010, bytes: 70033, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741829_1005, duration: 1120597176
2019-10-10 10:49:09,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2019-10-10 10:49:10,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741830_1006 src: /127.0.0.1:41874 dest: /127.0.0.1:50010
2019-10-10 10:49:11,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41874, dest: /127.0.0.1:50010, bytes: 66940, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741830_1006, duration: 1046661092
2019-10-10 10:49:11,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2019-10-10 10:49:11,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741831_1007 src: /127.0.0.1:41876 dest: /127.0.0.1:50010
2019-10-10 10:49:12,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41876, dest: /127.0.0.1:50010, bytes: 96290, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741831_1007, duration: 807791747
2019-10-10 10:49:12,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2019-10-10 10:49:12,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741832_1008 src: /127.0.0.1:41878 dest: /127.0.0.1:50010
2019-10-10 10:49:14,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41878, dest: /127.0.0.1:50010, bytes: 154939, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741832_1008, duration: 1926012255
2019-10-10 10:49:14,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2019-10-10 10:49:17,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741833_1009 src: /127.0.0.1:41882 dest: /127.0.0.1:50010
2019-10-10 10:49:18,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41882, dest: /127.0.0.1:50010, bytes: 65923, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741833_1009, duration: 1390195269
2019-10-10 10:49:18,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2019-10-10 10:49:21,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741834_1010 src: /127.0.0.1:41884 dest: /127.0.0.1:50010
2019-10-10 10:49:59,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41884, dest: /127.0.0.1:50010, bytes: 35844, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741834_1010, duration: 38040938318
2019-10-10 10:49:59,672 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:00,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741835_1011 src: /127.0.0.1:41906 dest: /127.0.0.1:50010
2019-10-10 10:50:01,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41906, dest: /127.0.0.1:50010, bytes: 96286, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741835_1011, duration: 747756554
2019-10-10 10:50:01,049 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:02,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741836_1012 src: /127.0.0.1:41908 dest: /127.0.0.1:50010
2019-10-10 10:50:03,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41908, dest: /127.0.0.1:50010, bytes: 98150, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741836_1012, duration: 1537339942
2019-10-10 10:50:03,621 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741836_1012, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:04,339 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741837_1013 src: /127.0.0.1:41910 dest: /127.0.0.1:50010
2019-10-10 10:50:05,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41910, dest: /127.0.0.1:50010, bytes: 70900, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741837_1013, duration: 1107382934
2019-10-10 10:50:05,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741837_1013, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:12,294 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741838_1014 src: /127.0.0.1:41914 dest: /127.0.0.1:50010
2019-10-10 10:50:16,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41914, dest: /127.0.0.1:50010, bytes: 29904, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741838_1014, duration: 4218465037
2019-10-10 10:50:16,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741838_1014, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:16,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741839_1015 src: /127.0.0.1:41916 dest: /127.0.0.1:50010
2019-10-10 10:50:17,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41916, dest: /127.0.0.1:50010, bytes: 74783, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741839_1015, duration: 1141634794
2019-10-10 10:50:17,869 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741839_1015, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:19,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741840_1016 src: /127.0.0.1:41918 dest: /127.0.0.1:50010
2019-10-10 10:50:20,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41918, dest: /127.0.0.1:50010, bytes: 94404, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741840_1016, duration: 1057550000
2019-10-10 10:50:20,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741840_1016, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:24,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741841_1017 src: /127.0.0.1:41920 dest: /127.0.0.1:50010
2019-10-10 10:50:26,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41920, dest: /127.0.0.1:50010, bytes: 102093, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741841_1017, duration: 1402006484
2019-10-10 10:50:26,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741841_1017, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:29,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741842_1018 src: /127.0.0.1:41922 dest: /127.0.0.1:50010
2019-10-10 10:50:31,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41922, dest: /127.0.0.1:50010, bytes: 136248, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741842_1018, duration: 1898892419
2019-10-10 10:50:31,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741842_1018, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:32,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741843_1019 src: /127.0.0.1:41924 dest: /127.0.0.1:50010
2019-10-10 10:50:34,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41924, dest: /127.0.0.1:50010, bytes: 77935, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741843_1019, duration: 1407137147
2019-10-10 10:50:34,244 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741843_1019, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:35,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741844_1020 src: /127.0.0.1:41926 dest: /127.0.0.1:50010
2019-10-10 10:50:37,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41926, dest: /127.0.0.1:50010, bytes: 67776, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741844_1020, duration: 1572783303
2019-10-10 10:50:37,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741844_1020, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:38,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741845_1021 src: /127.0.0.1:41928 dest: /127.0.0.1:50010
2019-10-10 10:50:39,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41928, dest: /127.0.0.1:50010, bytes: 73812, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741845_1021, duration: 1157785419
2019-10-10 10:50:39,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741845_1021, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:40,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741846_1022 src: /127.0.0.1:41930 dest: /127.0.0.1:50010
2019-10-10 10:50:41,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41930, dest: /127.0.0.1:50010, bytes: 68012, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741846_1022, duration: 1090668156
2019-10-10 10:50:41,822 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741846_1022, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:43,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741847_1023 src: /127.0.0.1:41932 dest: /127.0.0.1:50010
2019-10-10 10:50:44,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41932, dest: /127.0.0.1:50010, bytes: 101243, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741847_1023, duration: 982397811
2019-10-10 10:50:44,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741847_1023, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:47,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741848_1024 src: /127.0.0.1:41934 dest: /127.0.0.1:50010
2019-10-10 10:50:48,178 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41934, dest: /127.0.0.1:50010, bytes: 84401, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741848_1024, duration: 1115344494
2019-10-10 10:50:48,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741848_1024, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:53,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741849_1025 src: /127.0.0.1:41936 dest: /127.0.0.1:50010
2019-10-10 10:50:54,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41936, dest: /127.0.0.1:50010, bytes: 111626, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741849_1025, duration: 1465112128
2019-10-10 10:50:54,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741849_1025, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:54,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741850_1026 src: /127.0.0.1:41938 dest: /127.0.0.1:50010
2019-10-10 10:50:55,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41938, dest: /127.0.0.1:50010, bytes: 78358, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741850_1026, duration: 971495259
2019-10-10 10:50:55,833 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741850_1026, type=LAST_IN_PIPELINE terminating
2019-10-10 10:50:58,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741851_1027 src: /127.0.0.1:41940 dest: /127.0.0.1:50010
2019-10-10 10:50:59,756 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41940, dest: /127.0.0.1:50010, bytes: 75445, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741851_1027, duration: 1615910844
2019-10-10 10:50:59,757 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741851_1027, type=LAST_IN_PIPELINE terminating
2019-10-10 10:51:00,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741852_1028 src: /127.0.0.1:41944 dest: /127.0.0.1:50010
2019-10-10 10:51:02,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41944, dest: /127.0.0.1:50010, bytes: 80428, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741852_1028, duration: 1115794937
2019-10-10 10:51:02,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741852_1028, type=LAST_IN_PIPELINE terminating
2019-10-10 10:51:04,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741853_1029 src: /127.0.0.1:41946 dest: /127.0.0.1:50010
2019-10-10 10:51:06,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41946, dest: /127.0.0.1:50010, bytes: 83559, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741853_1029, duration: 2448595928
2019-10-10 10:51:06,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741853_1029, type=LAST_IN_PIPELINE terminating
2019-10-10 10:51:07,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741854_1030 src: /127.0.0.1:41948 dest: /127.0.0.1:50010
2019-10-10 10:51:08,886 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41948, dest: /127.0.0.1:50010, bytes: 86283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741854_1030, duration: 1133634579
2019-10-10 10:51:08,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741854_1030, type=LAST_IN_PIPELINE terminating
2019-10-10 10:51:12,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741855_1031 src: /127.0.0.1:41950 dest: /127.0.0.1:50010
2019-10-10 10:51:13,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41950, dest: /127.0.0.1:50010, bytes: 70424, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741855_1031, duration: 1273317010
2019-10-10 10:51:13,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741855_1031, type=LAST_IN_PIPELINE terminating
2019-10-10 10:51:14,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741856_1032 src: /127.0.0.1:41952 dest: /127.0.0.1:50010
2019-10-10 10:51:16,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41952, dest: /127.0.0.1:50010, bytes: 83261, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741856_1032, duration: 1314966813
2019-10-10 10:51:16,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741856_1032, type=LAST_IN_PIPELINE terminating
2019-10-10 10:51:17,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741857_1033 src: /127.0.0.1:41954 dest: /127.0.0.1:50010
2019-10-10 11:00:26,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:41954, dest: /127.0.0.1:50010, bytes: 14110546, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_276934315_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741857_1033, duration: 549129623434
2019-10-10 11:00:26,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741857_1033, type=LAST_IN_PIPELINE terminating
2019-10-10 11:02:55,866 INFO datanode.webhdfs: 127.0.0.1 GET /webhdfs/v1/tweets/FlumeData.1570718916190?op=OPEN&namenoderpcaddress=localhost:9000&offset=0 200
2019-10-10 11:21:30,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741858_1034 src: /127.0.0.1:42434 dest: /127.0.0.1:50010
2019-10-10 11:21:30,871 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42434, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786561744_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741858_1034, duration: 9696925
2019-10-10 11:21:30,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741858_1034, type=LAST_IN_PIPELINE terminating
2019-10-10 11:21:31,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741859_1035 src: /127.0.0.1:42436 dest: /127.0.0.1:50010
2019-10-10 11:21:31,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42436, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786561744_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741859_1035, duration: 1318079
2019-10-10 11:21:31,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741859_1035, type=LAST_IN_PIPELINE terminating
2019-10-10 11:21:32,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741860_1036 src: /127.0.0.1:42438 dest: /127.0.0.1:50010
2019-10-10 11:21:32,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42438, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786561744_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741860_1036, duration: 2760336
2019-10-10 11:21:32,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741860_1036, type=LAST_IN_PIPELINE terminating
2019-10-10 11:21:33,198 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741861_1037 src: /127.0.0.1:42440 dest: /127.0.0.1:50010
2019-10-10 11:21:33,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42440, dest: /127.0.0.1:50010, bytes: 135170, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1786561744_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741861_1037, duration: 7768133
2019-10-10 11:21:33,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741861_1037, type=LAST_IN_PIPELINE terminating
2019-10-10 11:30:36,905 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1876871648-127.0.1.1-1570716796840 Total blocks: 36, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-10-10 11:42:00,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741862_1038 src: /127.0.0.1:42768 dest: /127.0.0.1:50010
2019-10-10 11:42:00,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42768, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1499812647_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741862_1038, duration: 10797987
2019-10-10 11:42:00,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741862_1038, type=LAST_IN_PIPELINE terminating
2019-10-10 11:42:00,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741863_1039 src: /127.0.0.1:42774 dest: /127.0.0.1:50010
2019-10-10 11:42:00,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42774, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1499812647_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741863_1039, duration: 1494607
2019-10-10 11:42:00,261 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741863_1039, type=LAST_IN_PIPELINE terminating
2019-10-10 11:42:00,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741864_1040 src: /127.0.0.1:42776 dest: /127.0.0.1:50010
2019-10-10 11:42:00,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42776, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1499812647_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741864_1040, duration: 1364995
2019-10-10 11:42:00,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741864_1040, type=LAST_IN_PIPELINE terminating
2019-10-10 11:42:00,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741865_1041 src: /127.0.0.1:42778 dest: /127.0.0.1:50010
2019-10-10 11:42:00,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42778, dest: /127.0.0.1:50010, bytes: 135170, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1499812647_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741865_1041, duration: 1498922
2019-10-10 11:42:00,416 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741865_1041, type=LAST_IN_PIPELINE terminating
2019-10-10 11:46:56,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741866_1042 src: /127.0.0.1:42896 dest: /127.0.0.1:50010
2019-10-10 11:46:56,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42896, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_70312929_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741866_1042, duration: 11320090
2019-10-10 11:46:56,635 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741866_1042, type=LAST_IN_PIPELINE terminating
2019-10-10 11:46:57,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741867_1043 src: /127.0.0.1:42898 dest: /127.0.0.1:50010
2019-10-10 11:46:57,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42898, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_70312929_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741867_1043, duration: 2281650
2019-10-10 11:46:57,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741867_1043, type=LAST_IN_PIPELINE terminating
2019-10-10 11:46:58,106 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741868_1044 src: /127.0.0.1:42900 dest: /127.0.0.1:50010
2019-10-10 11:46:58,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42900, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_70312929_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741868_1044, duration: 1674598
2019-10-10 11:46:58,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741868_1044, type=LAST_IN_PIPELINE terminating
2019-10-10 11:46:58,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741869_1045 src: /127.0.0.1:42902 dest: /127.0.0.1:50010
2019-10-10 11:46:58,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42902, dest: /127.0.0.1:50010, bytes: 135177, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_70312929_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741869_1045, duration: 1756733
2019-10-10 11:46:58,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741869_1045, type=LAST_IN_PIPELINE terminating
2019-10-10 11:50:43,149 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 11:50:46,288 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 11:50:46,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 11:51:32,104 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 11:51:32,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 11:51:32,417 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 11:51:32,768 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 11:51:32,819 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 11:51:32,819 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 11:51:32,825 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 11:51:32,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 11:51:32,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 11:51:32,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 11:51:32,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 11:51:32,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 11:51:32,940 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 11:51:32,947 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 11:51:32,961 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 11:51:32,965 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 11:51:32,967 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 11:51:32,967 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 11:51:32,967 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 11:51:32,980 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 33903
2019-10-10 11:51:32,980 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 11:51:33,095 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:33903
2019-10-10 11:51:33,364 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 11:51:33,366 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 11:51:33,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 11:51:33,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 11:51:33,471 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 11:51:33,481 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 11:51:33,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 11:51:33,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 11:51:33,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 11:51:33,558 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 11:51:33,567 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 11:51:33,568 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 11:51:33,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 11:51:33,813 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 11:51:33,860 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 16712@H6
2019-10-10 11:51:34,062 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 11:51:34,062 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 11:51:34,127 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 11:51:34,238 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 11:51:34,238 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 11:51:34,254 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 11:51:34,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 11:51:34,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 11:51:34,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 11:51:34,336 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current: 17406028
2019-10-10 11:51:34,339 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 70ms
2019-10-10 11:51:34,339 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 80ms
2019-10-10 11:51:34,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 11:51:34,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 11:51:34,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 10ms
2019-10-10 11:51:34,351 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 10ms
2019-10-10 11:51:34,460 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1808537406 ms.
2019-10-10 11:51:34,466 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 3:02 PM with interval of 21600000ms
2019-10-10 11:51:34,468 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 11:51:34,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 11:51:34,508 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 11:51:34,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x1a69cca68f5904f3,  containing 1 storage report(s), of which we sent 1. The reports had 44 total blocks and used 1 RPC(s). This took 4 msec to generate and 53 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 11:51:34,717 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 11:55:23,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741870_1046 src: /127.0.0.1:43166 dest: /127.0.0.1:50010
2019-10-10 11:55:23,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43166, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1270318743_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741870_1046, duration: 11663146
2019-10-10 11:55:23,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741870_1046, type=LAST_IN_PIPELINE terminating
2019-10-10 11:55:24,303 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741871_1047 src: /127.0.0.1:43168 dest: /127.0.0.1:50010
2019-10-10 11:55:24,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43168, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1270318743_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741871_1047, duration: 1565376
2019-10-10 11:55:24,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741871_1047, type=LAST_IN_PIPELINE terminating
2019-10-10 11:55:24,504 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741872_1048 src: /127.0.0.1:43170 dest: /127.0.0.1:50010
2019-10-10 11:55:24,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43170, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1270318743_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741872_1048, duration: 9261049
2019-10-10 11:55:24,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741872_1048, type=LAST_IN_PIPELINE terminating
2019-10-10 11:55:24,853 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741873_1049 src: /127.0.0.1:43172 dest: /127.0.0.1:50010
2019-10-10 11:55:24,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43172, dest: /127.0.0.1:50010, bytes: 135174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1270318743_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741873_1049, duration: 9802607
2019-10-10 11:55:24,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741873_1049, type=LAST_IN_PIPELINE terminating
2019-10-10 12:57:35,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741874_1050 src: /127.0.0.1:43696 dest: /127.0.0.1:50010
2019-10-10 12:57:35,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43696, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_425226786_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741874_1050, duration: 19960954
2019-10-10 12:57:35,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741874_1050, type=LAST_IN_PIPELINE terminating
2019-10-10 12:57:36,003 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741875_1051 src: /127.0.0.1:43698 dest: /127.0.0.1:50010
2019-10-10 12:57:36,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43698, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_425226786_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741875_1051, duration: 9009753
2019-10-10 12:57:36,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741875_1051, type=LAST_IN_PIPELINE terminating
2019-10-10 12:57:36,272 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741876_1052 src: /127.0.0.1:43700 dest: /127.0.0.1:50010
2019-10-10 12:57:36,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43700, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_425226786_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741876_1052, duration: 5425910
2019-10-10 12:57:36,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741876_1052, type=LAST_IN_PIPELINE terminating
2019-10-10 12:57:36,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741877_1053 src: /127.0.0.1:43702 dest: /127.0.0.1:50010
2019-10-10 12:57:36,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43702, dest: /127.0.0.1:50010, bytes: 135174, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_425226786_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741877_1053, duration: 8894805
2019-10-10 12:57:36,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741877_1053, type=LAST_IN_PIPELINE terminating
2019-10-10 13:13:17,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741878_1054 src: /127.0.0.1:44036 dest: /127.0.0.1:50010
2019-10-10 13:13:17,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44036, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-56247652_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741878_1054, duration: 11915871
2019-10-10 13:13:17,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741878_1054, type=LAST_IN_PIPELINE terminating
2019-10-10 13:13:17,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741879_1055 src: /127.0.0.1:44038 dest: /127.0.0.1:50010
2019-10-10 13:13:17,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44038, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-56247652_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741879_1055, duration: 3188750
2019-10-10 13:13:17,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741879_1055, type=LAST_IN_PIPELINE terminating
2019-10-10 13:13:17,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741880_1056 src: /127.0.0.1:44040 dest: /127.0.0.1:50010
2019-10-10 13:13:17,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44040, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-56247652_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741880_1056, duration: 2236265
2019-10-10 13:13:17,748 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741880_1056, type=LAST_IN_PIPELINE terminating
2019-10-10 13:13:17,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741881_1057 src: /127.0.0.1:44042 dest: /127.0.0.1:50010
2019-10-10 13:13:17,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:44042, dest: /127.0.0.1:50010, bytes: 135204, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-56247652_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741881_1057, duration: 16752316
2019-10-10 13:13:17,901 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741881_1057, type=LAST_IN_PIPELINE terminating
2019-10-10 13:14:43,660 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 13:14:46,575 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 13:14:46,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 13:27:05,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 13:27:05,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 13:27:06,099 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 13:27:06,480 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 13:27:06,541 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 13:27:06,541 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 13:27:06,561 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 13:27:06,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 13:27:06,565 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 13:27:06,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 13:27:06,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 13:27:06,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 13:27:06,707 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 13:27:06,715 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 13:27:06,729 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 13:27:06,734 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 13:27:06,736 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 13:27:06,736 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 13:27:06,736 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 13:27:06,778 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39701
2019-10-10 13:27:06,779 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 13:27:06,995 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39701
2019-10-10 13:27:07,260 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 13:27:07,262 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 13:27:07,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 13:27:07,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 13:27:07,366 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 13:27:07,377 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 13:27:07,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 13:27:07,438 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 13:27:07,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 13:27:07,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 13:27:07,463 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 13:27:07,469 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 13:27:07,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 13:27:07,670 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 13:27:07,702 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 21777@H6
2019-10-10 13:27:07,878 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:27:07,878 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:27:07,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 13:27:08,018 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 13:27:08,018 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 13:27:08,023 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 13:27:08,029 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 13:27:08,029 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:27:08,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 13:27:08,093 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 47ms
2019-10-10 13:27:08,094 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 65ms
2019-10-10 13:27:08,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 13:27:08,096 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 13:27:08,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 13ms
2019-10-10 13:27:08,108 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 13ms
2019-10-10 13:27:08,276 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1802803590 ms.
2019-10-10 13:27:08,315 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 1:33 PM with interval of 21600000ms
2019-10-10 13:27:08,318 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 13:27:08,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 13:27:08,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 13:27:08,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x45babe17733c6c97,  containing 1 storage report(s), of which we sent 1. The reports had 56 total blocks and used 1 RPC(s). This took 4 msec to generate and 45 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 13:27:08,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:30:21,450 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 13:30:21,453 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 13:35:11,473 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 13:35:11,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 13:35:11,907 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 13:35:14,800 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 13:35:14,864 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 13:35:14,864 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 13:35:14,955 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 13:35:15,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 13:35:15,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 13:35:15,866 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 13:35:15,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 13:35:15,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 13:35:16,467 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 13:35:16,475 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 13:35:16,488 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 13:35:16,494 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 13:35:16,528 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 13:35:16,529 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 13:35:16,529 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 13:35:16,757 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35487
2019-10-10 13:35:16,757 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 13:35:16,887 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35487
2019-10-10 13:35:20,260 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 13:35:20,262 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 13:35:20,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 13:35:20,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 13:35:20,942 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 13:35:21,208 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 13:35:22,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 13:35:22,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 13:35:22,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 13:35:22,661 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 13:35:22,804 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 13:35:22,807 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 13:35:24,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 13:35:25,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 13:35:26,667 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 13:35:27,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 13:35:27,136 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 13:35:27,266 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 3361@H6
2019-10-10 13:35:27,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:35:27,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:35:27,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 13:35:27,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 13:35:27,897 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 13:35:27,901 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 13:35:27,906 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 13:35:27,906 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:35:27,952 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 13:35:28,153 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current: 17952768
2019-10-10 13:35:28,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 196ms
2019-10-10 13:35:28,156 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 250ms
2019-10-10 13:35:28,159 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 13:35:28,159 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 13:35:28,174 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 15ms
2019-10-10 13:35:28,174 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 16ms
2019-10-10 13:35:28,274 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1802303592 ms.
2019-10-10 13:35:28,279 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 3:57 PM with interval of 21600000ms
2019-10-10 13:35:28,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 13:35:28,319 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 13:35:28,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 13:35:29,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x6e6c826eb466cdc8,  containing 1 storage report(s), of which we sent 1. The reports had 56 total blocks and used 1 RPC(s). This took 26 msec to generate and 55 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 13:35:29,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 13:38:42,397 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741882_1058 src: /127.0.0.1:39722 dest: /127.0.0.1:50010
2019-10-10 13:38:42,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39722, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1385937687_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741882_1058, duration: 18717971
2019-10-10 13:38:42,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741882_1058, type=LAST_IN_PIPELINE terminating
2019-10-10 13:38:43,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741883_1059 src: /127.0.0.1:39724 dest: /127.0.0.1:50010
2019-10-10 13:38:43,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39724, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1385937687_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741883_1059, duration: 3116824
2019-10-10 13:38:43,550 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741883_1059, type=LAST_IN_PIPELINE terminating
2019-10-10 13:38:43,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741884_1060 src: /127.0.0.1:39726 dest: /127.0.0.1:50010
2019-10-10 13:38:43,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39726, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1385937687_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741884_1060, duration: 2158077
2019-10-10 13:38:43,810 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741884_1060, type=LAST_IN_PIPELINE terminating
2019-10-10 13:38:45,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741885_1061 src: /127.0.0.1:39728 dest: /127.0.0.1:50010
2019-10-10 13:38:45,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:39728, dest: /127.0.0.1:50010, bytes: 135189, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1385937687_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741885_1061, duration: 11526456
2019-10-10 13:38:45,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741885_1061, type=LAST_IN_PIPELINE terminating
2019-10-10 13:53:16,482 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 13:53:20,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 13:53:21,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 13:53:21,524 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 13:53:21,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 14:02:05,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 14:02:05,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 14:02:05,591 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 14:02:05,972 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 14:02:06,030 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 14:02:06,030 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 14:02:06,035 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 14:02:06,036 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 14:02:06,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 14:02:06,057 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 14:02:06,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 14:02:06,059 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 14:02:06,161 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 14:02:06,168 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 14:02:06,181 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 14:02:06,186 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 14:02:06,187 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 14:02:06,187 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 14:02:06,187 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 14:02:06,199 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45583
2019-10-10 14:02:06,200 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 14:02:06,323 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45583
2019-10-10 14:02:06,607 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 14:02:06,610 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 14:02:06,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 14:02:06,634 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 14:02:06,695 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 14:02:06,706 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 14:02:06,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 14:02:06,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 14:02:06,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 14:02:06,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 14:02:06,789 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 14:02:06,790 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 14:02:06,992 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 14:02:06,994 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 14:02:07,023 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 8449@H6
2019-10-10 14:02:07,160 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:02:07,160 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:02:07,231 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 14:02:07,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 14:02:07,270 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 14:02:07,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 14:02:07,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 14:02:07,282 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:02:07,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 14:02:07,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current: 18124800
2019-10-10 14:02:07,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 29ms
2019-10-10 14:02:07,313 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 31ms
2019-10-10 14:02:07,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 14:02:07,315 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 14:02:07,329 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 14ms
2019-10-10 14:02:07,329 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 15ms
2019-10-10 14:02:07,402 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1800704464 ms.
2019-10-10 14:02:07,409 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 5:51 PM with interval of 21600000ms
2019-10-10 14:02:07,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 14:02:07,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 14:02:07,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 14:02:07,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xdad371439f14dfc7,  containing 1 storage report(s), of which we sent 1. The reports had 60 total blocks and used 1 RPC(s). This took 3 msec to generate and 53 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 14:02:07,562 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:03:40,410 INFO logs: Aliases are enabled
2019-10-10 14:12:04,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741886_1062 src: /127.0.0.1:40650 dest: /127.0.0.1:50010
2019-10-10 14:12:04,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40650, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1850088851_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741886_1062, duration: 15111465
2019-10-10 14:12:04,784 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741886_1062, type=LAST_IN_PIPELINE terminating
2019-10-10 14:12:05,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741887_1063 src: /127.0.0.1:40652 dest: /127.0.0.1:50010
2019-10-10 14:12:05,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40652, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1850088851_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741887_1063, duration: 2421266
2019-10-10 14:12:05,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741887_1063, type=LAST_IN_PIPELINE terminating
2019-10-10 14:12:05,812 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741888_1064 src: /127.0.0.1:40654 dest: /127.0.0.1:50010
2019-10-10 14:12:05,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40654, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1850088851_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741888_1064, duration: 2079906
2019-10-10 14:12:05,816 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741888_1064, type=LAST_IN_PIPELINE terminating
2019-10-10 14:12:06,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741889_1065 src: /127.0.0.1:40656 dest: /127.0.0.1:50010
2019-10-10 14:12:06,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:40656, dest: /127.0.0.1:50010, bytes: 135156, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1850088851_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741889_1065, duration: 5963334
2019-10-10 14:12:06,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741889_1065, type=LAST_IN_PIPELINE terminating
2019-10-10 14:18:25,494 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 14:18:28,832 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 14:18:28,835 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 14:32:56,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 14:32:56,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 14:32:57,198 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 14:32:57,794 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 14:32:57,845 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 14:32:57,845 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 14:32:57,857 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 14:32:57,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 14:32:57,865 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 14:32:57,953 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 14:32:57,956 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 14:32:57,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 14:32:58,170 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 14:32:58,176 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 14:32:58,189 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 14:32:58,194 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 14:32:58,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 14:32:58,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 14:32:58,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 14:32:58,262 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 34033
2019-10-10 14:32:58,262 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 14:32:58,416 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34033
2019-10-10 14:33:00,076 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 14:33:00,078 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 14:33:00,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 14:33:00,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 14:33:00,328 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 14:33:00,370 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 14:33:00,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 14:33:00,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 14:33:00,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 14:33:00,764 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 14:33:00,794 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 14:33:00,814 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 14:33:01,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 14:33:01,363 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 14:33:01,449 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 4035@H6
2019-10-10 14:33:01,653 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:33:01,654 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:33:01,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 14:33:01,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 14:33:01,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 14:33:01,798 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 14:33:01,807 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 14:33:01,808 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:33:01,830 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 14:33:01,968 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 138ms
2019-10-10 14:33:01,968 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 161ms
2019-10-10 14:33:01,970 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 14:33:01,970 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 14:33:01,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 13ms
2019-10-10 14:33:01,983 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 14ms
2019-10-10 14:33:02,043 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1798849823 ms.
2019-10-10 14:33:02,050 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 6:53 PM with interval of 21600000ms
2019-10-10 14:33:02,052 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 14:33:02,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 14:33:02,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 14:33:02,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x2040f555b92b5a0b,  containing 1 storage report(s), of which we sent 1. The reports had 64 total blocks and used 1 RPC(s). This took 7 msec to generate and 62 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 14:33:02,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 14:35:04,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741890_1066 src: /127.0.0.1:49698 dest: /127.0.0.1:50010
2019-10-10 14:35:04,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49698, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327120685_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741890_1066, duration: 17503272
2019-10-10 14:35:04,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741890_1066, type=LAST_IN_PIPELINE terminating
2019-10-10 14:35:04,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741891_1067 src: /127.0.0.1:49700 dest: /127.0.0.1:50010
2019-10-10 14:35:04,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49700, dest: /127.0.0.1:50010, bytes: 3634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327120685_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741891_1067, duration: 6791510
2019-10-10 14:35:04,779 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741891_1067, type=LAST_IN_PIPELINE terminating
2019-10-10 14:35:04,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741892_1068 src: /127.0.0.1:49702 dest: /127.0.0.1:50010
2019-10-10 14:35:04,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49702, dest: /127.0.0.1:50010, bytes: 343, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327120685_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741892_1068, duration: 2360156
2019-10-10 14:35:04,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741892_1068, type=LAST_IN_PIPELINE terminating
2019-10-10 14:35:05,454 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741893_1069 src: /127.0.0.1:49704 dest: /127.0.0.1:50010
2019-10-10 14:35:05,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49704, dest: /127.0.0.1:50010, bytes: 135363, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1327120685_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741893_1069, duration: 8216688
2019-10-10 14:35:05,464 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741893_1069, type=LAST_IN_PIPELINE terminating
2019-10-10 14:43:26,216 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-10-10 14:43:26,241 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2019-10-10 14:43:26,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-10-10 14:43:26,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2019-10-10 14:43:26,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741832_1008 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741832 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741833_1009 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741833 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741834_1010 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741834 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2019-10-10 14:43:26,259 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741841_1017 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741841 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741842_1018 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741842 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741843_1019 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741843 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741850_1026 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741850 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741851_1027 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741851 for deletion
2019-10-10 14:43:26,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741852_1028 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741852 for deletion
2019-10-10 14:43:26,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2019-10-10 14:43:26,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2019-10-10 14:43:26,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2019-10-10 14:43:26,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2019-10-10 14:43:26,261 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2019-10-10 14:43:26,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741827_1003 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741827
2019-10-10 14:43:26,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741828_1004 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741828
2019-10-10 14:43:26,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741829_1005 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741829
2019-10-10 14:43:26,262 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741830_1006 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741830
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741831_1007 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741831
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741832_1008 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741832
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741833_1009 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741833
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741834_1010 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741834
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741835_1011 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741835
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741836_1012 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741836
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741837_1013 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741837
2019-10-10 14:43:26,263 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741838_1014 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741838
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741839_1015 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741839
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741840_1016 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741840
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741841_1017 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741841
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741842_1018 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741842
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741843_1019 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741843
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741844_1020 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741844
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741845_1021 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741845
2019-10-10 14:43:26,264 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741846_1022 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741846
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741847_1023 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741847
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741848_1024 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741848
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741849_1025 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741849
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741850_1026 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741850
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741851_1027 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741851
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741852_1028 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741852
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741853_1029 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741853
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741854_1030 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741854
2019-10-10 14:43:26,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741855_1031 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741855
2019-10-10 14:43:26,266 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741856_1032 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741856
2019-10-10 14:43:26,294 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741857_1033 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741857
2019-10-10 14:44:46,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741894_1070 src: /127.0.0.1:49872 dest: /127.0.0.1:50010
2019-10-10 14:44:47,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49872, dest: /127.0.0.1:50010, bytes: 69573, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317446379_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741894_1070, duration: 1079673749
2019-10-10 14:44:47,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741894_1070, type=LAST_IN_PIPELINE terminating
2019-10-10 14:44:50,004 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741895_1071 src: /127.0.0.1:49874 dest: /127.0.0.1:50010
2019-10-10 14:44:50,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49874, dest: /127.0.0.1:50010, bytes: 71716, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317446379_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741895_1071, duration: 907118117
2019-10-10 14:44:50,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741895_1071, type=LAST_IN_PIPELINE terminating
2019-10-10 14:44:54,846 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741896_1072 src: /127.0.0.1:49876 dest: /127.0.0.1:50010
2019-10-10 14:44:55,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49876, dest: /127.0.0.1:50010, bytes: 67503, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317446379_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741896_1072, duration: 1042057010
2019-10-10 14:44:55,890 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741896_1072, type=LAST_IN_PIPELINE terminating
2019-10-10 14:44:57,971 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741897_1073 src: /127.0.0.1:49878 dest: /127.0.0.1:50010
2019-10-10 14:44:59,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49878, dest: /127.0.0.1:50010, bytes: 68269, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317446379_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741897_1073, duration: 1067100343
2019-10-10 14:44:59,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741897_1073, type=LAST_IN_PIPELINE terminating
2019-10-10 14:45:06,942 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741898_1074 src: /127.0.0.1:49882 dest: /127.0.0.1:50010
2019-10-10 14:45:06,958 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49882, dest: /127.0.0.1:50010, bytes: 18176, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1317446379_28, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741898_1074, duration: 12578439
2019-10-10 14:45:06,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741898_1074, type=LAST_IN_PIPELINE terminating
2019-10-10 14:45:41,132 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741888_1064 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741888 for deletion
2019-10-10 14:45:41,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741889_1065 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741889 for deletion
2019-10-10 14:45:41,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741890_1066 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741890 for deletion
2019-10-10 14:45:41,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741891_1067 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741891 for deletion
2019-10-10 14:45:41,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2019-10-10 14:45:41,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2019-10-10 14:45:41,133 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741860_1036 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741860 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741861_1037 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741861 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2019-10-10 14:45:41,134 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741869_1045 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741869 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741870_1046 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741870 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741873_1049 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741873 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741876_1052 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741876 for deletion
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741888_1064 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741888
2019-10-10 14:45:41,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741877_1053 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741877 for deletion
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741878_1054 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741878 for deletion
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741889_1065 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741889
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741879_1055 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741879 for deletion
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741890_1066 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741890
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741891_1067 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741891
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741882_1058 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741882 for deletion
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741892_1068 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741892
2019-10-10 14:45:41,136 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741883_1059 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741883 for deletion
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741893_1069 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741893
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741884_1060 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741884 for deletion
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741858_1034 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741858
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741885_1061 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741885 for deletion
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741859_1035 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741859
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741860_1036 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741860
2019-10-10 14:45:41,137 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741861_1037 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741861
2019-10-10 14:45:41,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741862_1038 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741862
2019-10-10 14:45:41,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741863_1039 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741863
2019-10-10 14:45:41,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741864_1040 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741864
2019-10-10 14:45:41,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741865_1041 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741865
2019-10-10 14:45:41,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741866_1042 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741866
2019-10-10 14:45:41,138 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741867_1043 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741867
2019-10-10 14:45:41,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741868_1044 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741868
2019-10-10 14:45:41,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741869_1045 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741869
2019-10-10 14:45:41,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741870_1046 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741870
2019-10-10 14:45:41,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741871_1047 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741871
2019-10-10 14:45:41,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741872_1048 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741872
2019-10-10 14:45:41,139 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741873_1049 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741873
2019-10-10 14:45:41,140 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741874_1050 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741874
2019-10-10 14:45:41,140 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741875_1051 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741875
2019-10-10 14:45:41,140 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741876_1052 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741876
2019-10-10 14:45:41,140 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741877_1053 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741877
2019-10-10 14:45:41,140 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741878_1054 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741878
2019-10-10 14:45:41,141 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741879_1055 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741879
2019-10-10 14:45:41,141 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741880_1056 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741880
2019-10-10 14:45:41,141 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741881_1057 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741881
2019-10-10 14:45:41,141 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741882_1058 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741882
2019-10-10 14:45:41,141 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741883_1059 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741883
2019-10-10 14:45:41,141 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741884_1060 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741884
2019-10-10 14:45:41,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741885_1061 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741885
2019-10-10 14:45:41,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741886_1062 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741886
2019-10-10 14:45:41,143 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741887_1063 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741887
2019-10-10 14:46:20,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741899_1075 src: /127.0.0.1:49916 dest: /127.0.0.1:50010
2019-10-10 14:46:20,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49916, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_825955283_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741899_1075, duration: 9613151
2019-10-10 14:46:20,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741899_1075, type=LAST_IN_PIPELINE terminating
2019-10-10 14:46:20,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741900_1076 src: /127.0.0.1:49918 dest: /127.0.0.1:50010
2019-10-10 14:46:20,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49918, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_825955283_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741900_1076, duration: 2213819
2019-10-10 14:46:20,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741900_1076, type=LAST_IN_PIPELINE terminating
2019-10-10 14:46:20,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741901_1077 src: /127.0.0.1:49920 dest: /127.0.0.1:50010
2019-10-10 14:46:20,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49920, dest: /127.0.0.1:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_825955283_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741901_1077, duration: 1491251
2019-10-10 14:46:20,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741901_1077, type=LAST_IN_PIPELINE terminating
2019-10-10 14:46:21,148 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741902_1078 src: /127.0.0.1:49922 dest: /127.0.0.1:50010
2019-10-10 14:46:21,164 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:49922, dest: /127.0.0.1:50010, bytes: 135368, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_825955283_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741902_1078, duration: 13701991
2019-10-10 14:46:21,165 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741902_1078, type=LAST_IN_PIPELINE terminating
2019-10-10 15:08:05,185 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 15:08:08,248 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 15:08:08,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 15:16:54,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 15:16:54,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 15:16:54,550 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 15:16:54,877 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 15:16:54,933 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 15:16:54,933 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 15:16:54,939 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 15:16:54,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 15:16:54,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 15:16:54,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 15:16:54,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 15:16:54,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 15:16:55,041 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 15:16:55,048 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 15:16:55,065 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 15:16:55,070 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 15:16:55,073 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 15:16:55,073 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 15:16:55,073 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 15:16:55,089 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 43997
2019-10-10 15:16:55,089 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 15:16:55,232 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43997
2019-10-10 15:16:55,777 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 15:16:55,780 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 15:16:55,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 15:16:55,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 15:16:55,869 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 15:16:55,880 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 15:16:55,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 15:16:55,934 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 15:16:55,941 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 15:16:55,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 15:16:55,963 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 15:16:55,970 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 15:16:56,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 15:16:56,165 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 15:16:56,197 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 8945@H6
2019-10-10 15:16:56,270 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:16:56,270 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:16:56,322 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 15:16:56,375 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 15:16:56,376 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 15:16:56,380 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 15:16:56,387 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 15:16:56,387 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:16:56,388 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:16:56,422 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current: 569344
2019-10-10 15:16:56,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 49ms
2019-10-10 15:16:56,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 51ms
2019-10-10 15:16:56,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:16:56,458 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 15:16:56,464 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 5ms
2019-10-10 15:16:56,472 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 33ms
2019-10-10 15:16:56,575 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1796215291 ms.
2019-10-10 15:16:56,582 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 4:00 PM with interval of 21600000ms
2019-10-10 15:16:56,585 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 15:16:56,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 15:16:56,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 15:16:56,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x39350ba9ba7d0d4f,  containing 1 storage report(s), of which we sent 1. The reports had 10 total blocks and used 1 RPC(s). This took 4 msec to generate and 44 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 15:16:56,726 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:17:56,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741903_1079 src: /127.0.0.1:51378 dest: /127.0.0.1:50010
2019-10-10 15:17:56,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51378, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-539247209_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741903_1079, duration: 14379298
2019-10-10 15:17:56,658 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741903_1079, type=LAST_IN_PIPELINE terminating
2019-10-10 15:17:56,791 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741904_1080 src: /127.0.0.1:51380 dest: /127.0.0.1:50010
2019-10-10 15:17:56,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51380, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-539247209_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741904_1080, duration: 1524999
2019-10-10 15:17:56,796 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741904_1080, type=LAST_IN_PIPELINE terminating
2019-10-10 15:17:56,840 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741905_1081 src: /127.0.0.1:51382 dest: /127.0.0.1:50010
2019-10-10 15:17:56,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51382, dest: /127.0.0.1:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-539247209_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741905_1081, duration: 9334938
2019-10-10 15:17:56,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741905_1081, type=LAST_IN_PIPELINE terminating
2019-10-10 15:17:57,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741906_1082 src: /127.0.0.1:51384 dest: /127.0.0.1:50010
2019-10-10 15:17:57,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51384, dest: /127.0.0.1:50010, bytes: 135360, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-539247209_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741906_1082, duration: 7843082
2019-10-10 15:17:57,541 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741906_1082, type=LAST_IN_PIPELINE terminating
2019-10-10 15:23:26,640 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 15:23:30,317 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 15:23:30,327 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 15:25:17,787 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 15:25:17,794 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 15:25:18,116 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 15:25:18,388 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 15:25:18,441 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 15:25:18,441 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 15:25:18,446 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 15:25:18,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 15:25:18,450 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 15:25:18,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 15:25:18,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 15:25:18,469 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 15:25:18,528 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 15:25:18,535 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 15:25:18,548 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 15:25:18,553 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 15:25:18,555 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 15:25:18,555 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 15:25:18,555 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 15:25:18,567 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38565
2019-10-10 15:25:18,567 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 15:25:18,683 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38565
2019-10-10 15:25:18,837 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 15:25:18,846 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 15:25:18,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 15:25:18,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 15:25:18,946 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 15:25:18,958 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 15:25:19,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 15:25:19,011 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 15:25:19,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 15:25:19,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 15:25:19,042 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 15:25:19,043 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 15:25:19,247 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 15:25:19,249 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 15:25:19,286 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 11574@H6
2019-10-10 15:25:19,426 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:25:19,426 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:25:19,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 15:25:19,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 15:25:19,512 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 15:25:19,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 15:25:19,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 15:25:19,526 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:25:19,527 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:25:19,535 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current: 711661
2019-10-10 15:25:19,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 11ms
2019-10-10 15:25:19,538 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 12ms
2019-10-10 15:25:19,541 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:25:19,542 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 15:25:19,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 12ms
2019-10-10 15:25:19,553 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 13ms
2019-10-10 15:25:19,603 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1795712263 ms.
2019-10-10 15:25:19,612 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 6:55 PM with interval of 21600000ms
2019-10-10 15:25:19,616 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 15:25:19,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 15:25:19,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 15:25:19,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x4e233794f31339ab,  containing 1 storage report(s), of which we sent 1. The reports had 14 total blocks and used 1 RPC(s). This took 4 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 15:25:19,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:25:52,773 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741907_1083 src: /127.0.0.1:51738 dest: /127.0.0.1:50010
2019-10-10 15:25:52,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51738, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_715514875_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741907_1083, duration: 16374077
2019-10-10 15:25:52,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741907_1083, type=LAST_IN_PIPELINE terminating
2019-10-10 15:25:53,343 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741908_1084 src: /127.0.0.1:51740 dest: /127.0.0.1:50010
2019-10-10 15:25:53,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51740, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_715514875_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741908_1084, duration: 1995218
2019-10-10 15:25:53,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741908_1084, type=LAST_IN_PIPELINE terminating
2019-10-10 15:25:53,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741909_1085 src: /127.0.0.1:51742 dest: /127.0.0.1:50010
2019-10-10 15:25:53,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51742, dest: /127.0.0.1:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_715514875_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741909_1085, duration: 2275109
2019-10-10 15:25:53,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741909_1085, type=LAST_IN_PIPELINE terminating
2019-10-10 15:25:53,960 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741910_1086 src: /127.0.0.1:51744 dest: /127.0.0.1:50010
2019-10-10 15:25:53,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51744, dest: /127.0.0.1:50010, bytes: 135514, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_715514875_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741910_1086, duration: 19052536
2019-10-10 15:25:53,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741910_1086, type=LAST_IN_PIPELINE terminating
2019-10-10 15:30:40,690 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 15:30:43,330 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 15:30:43,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 15:35:01,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 15:35:01,415 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 15:35:01,757 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 15:35:02,063 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 15:35:02,122 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 15:35:02,122 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 15:35:02,127 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 15:35:02,128 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 15:35:02,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 15:35:02,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 15:35:02,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 15:35:02,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 15:35:02,214 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 15:35:02,221 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 15:35:02,241 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 15:35:02,246 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 15:35:02,248 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 15:35:02,248 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 15:35:02,248 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 15:35:02,261 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42701
2019-10-10 15:35:02,261 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 15:35:02,395 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42701
2019-10-10 15:35:02,548 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 15:35:02,555 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 15:35:02,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 15:35:02,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 15:35:02,652 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 15:35:02,663 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 15:35:02,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 15:35:02,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 15:35:02,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 15:35:02,752 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 15:35:02,763 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 15:35:02,764 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 15:35:02,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 15:35:02,970 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 15:35:03,021 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 13837@H6
2019-10-10 15:35:03,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:35:03,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:35:03,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 15:35:03,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 15:35:03,239 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 15:35:03,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 15:35:03,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 15:35:03,250 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:35:03,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:35:03,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current: 854132
2019-10-10 15:35:03,268 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 17ms
2019-10-10 15:35:03,269 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 18ms
2019-10-10 15:35:03,275 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:35:03,276 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 15:35:03,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 9ms
2019-10-10 15:35:03,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 14ms
2019-10-10 15:35:03,348 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1795128518 ms.
2019-10-10 15:35:03,355 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 7:18 PM with interval of 21600000ms
2019-10-10 15:35:03,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 15:35:03,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 15:35:03,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 15:35:03,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x482826a3e4f6f63e,  containing 1 storage report(s), of which we sent 1. The reports had 18 total blocks and used 1 RPC(s). This took 5 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 15:35:03,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:45:15,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741911_1087 src: /127.0.0.1:52908 dest: /127.0.0.1:50010
2019-10-10 15:45:15,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52908, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-416556314_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741911_1087, duration: 20791380
2019-10-10 15:45:15,288 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741911_1087, type=LAST_IN_PIPELINE terminating
2019-10-10 15:45:16,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741912_1088 src: /127.0.0.1:52912 dest: /127.0.0.1:50010
2019-10-10 15:45:16,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52912, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-416556314_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741912_1088, duration: 21857266
2019-10-10 15:45:16,123 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741912_1088, type=LAST_IN_PIPELINE terminating
2019-10-10 15:45:16,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741913_1089 src: /127.0.0.1:52914 dest: /127.0.0.1:50010
2019-10-10 15:45:16,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52914, dest: /127.0.0.1:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-416556314_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741913_1089, duration: 8335235
2019-10-10 15:45:16,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741913_1089, type=LAST_IN_PIPELINE terminating
2019-10-10 15:45:17,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741914_1090 src: /127.0.0.1:52916 dest: /127.0.0.1:50010
2019-10-10 15:45:17,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52916, dest: /127.0.0.1:50010, bytes: 135366, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-416556314_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741914_1090, duration: 8964357
2019-10-10 15:45:17,320 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741914_1090, type=LAST_IN_PIPELINE terminating
2019-10-10 15:50:27,567 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 15:50:31,283 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 15:50:31,286 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 15:56:40,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 15:56:40,043 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 15:56:40,377 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 15:56:40,959 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 15:56:41,014 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 15:56:41,014 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 15:56:41,081 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 15:56:41,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 15:56:41,099 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 15:56:41,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 15:56:41,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 15:56:41,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 15:56:41,271 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 15:56:41,279 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 15:56:41,302 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 15:56:41,307 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 15:56:41,309 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 15:56:41,309 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 15:56:41,309 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 15:56:41,327 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 38215
2019-10-10 15:56:41,327 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 15:56:41,497 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:38215
2019-10-10 15:56:42,165 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 15:56:42,167 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 15:56:42,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 15:56:42,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 15:56:42,268 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 15:56:42,282 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 15:56:42,332 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 15:56:42,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 15:56:42,349 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 15:56:42,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 15:56:42,401 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 15:56:42,401 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 15:56:42,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 15:56:42,640 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 15:56:42,673 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 17542@H6
2019-10-10 15:56:42,936 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:56:42,936 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:56:43,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 15:56:43,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 15:56:43,201 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 15:56:43,278 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 15:56:43,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 15:56:43,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 15:56:43,335 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:56:43,652 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current: 1085440
2019-10-10 15:56:43,654 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 319ms
2019-10-10 15:56:43,654 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 370ms
2019-10-10 15:56:43,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 15:56:43,656 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 15:56:43,663 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 7ms
2019-10-10 15:56:43,664 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2019-10-10 15:56:43,805 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1793828061 ms.
2019-10-10 15:56:43,811 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 7:13 PM with interval of 21600000ms
2019-10-10 15:56:43,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 15:56:43,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 15:56:43,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 15:56:44,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xbb05c2e68518a25e,  containing 1 storage report(s), of which we sent 1. The reports had 22 total blocks and used 1 RPC(s). This took 4 msec to generate and 100 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 15:56:44,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 16:00:40,921 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 16:00:44,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-10 16:00:45,292 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 16:00:45,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 16:12:08,546 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 16:12:08,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 16:12:08,881 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 16:12:09,438 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 16:12:09,493 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 16:12:09,493 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 16:12:09,498 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 16:12:09,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 16:12:09,501 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 16:12:09,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 16:12:09,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 16:12:09,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 16:12:09,616 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 16:12:09,624 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 16:12:09,645 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 16:12:09,650 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 16:12:09,652 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 16:12:09,652 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 16:12:09,652 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 16:12:09,668 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 37585
2019-10-10 16:12:09,668 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 16:12:09,818 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:37585
2019-10-10 16:12:10,372 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 16:12:10,374 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 16:12:10,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 16:12:10,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 16:12:10,493 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 16:12:10,504 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 16:12:10,555 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 16:12:10,584 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 16:12:10,590 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 16:12:10,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 16:12:10,672 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 16:12:10,672 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 16:12:10,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 16:12:10,900 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 16:12:10,970 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 19668@H6
2019-10-10 16:12:11,103 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 16:12:11,103 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 16:12:11,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-10 16:12:11,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-10 16:12:11,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 16:12:11,207 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 16:12:11,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 16:12:11,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 16:12:11,215 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 16:12:11,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 109ms
2019-10-10 16:12:11,324 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 111ms
2019-10-10 16:12:11,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 16:12:11,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 16:12:11,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 7ms
2019-10-10 16:12:11,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2019-10-10 16:12:11,415 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1792900451 ms.
2019-10-10 16:12:11,421 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 7:13 PM with interval of 21600000ms
2019-10-10 16:12:11,423 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 16:12:11,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 16:12:11,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 16:12:11,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x83268e1c77f894f7,  containing 1 storage report(s), of which we sent 1. The reports had 22 total blocks and used 1 RPC(s). This took 4 msec to generate and 48 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 16:12:11,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 16:17:31,878 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741915_1091 src: /127.0.0.1:53720 dest: /127.0.0.1:50010
2019-10-10 16:17:31,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53720, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1127944426_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741915_1091, duration: 9868006
2019-10-10 16:17:31,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741915_1091, type=LAST_IN_PIPELINE terminating
2019-10-10 16:17:32,595 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741916_1092 src: /127.0.0.1:53722 dest: /127.0.0.1:50010
2019-10-10 16:17:32,605 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53722, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1127944426_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741916_1092, duration: 1755660
2019-10-10 16:17:32,606 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741916_1092, type=LAST_IN_PIPELINE terminating
2019-10-10 16:17:32,737 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741917_1093 src: /127.0.0.1:53724 dest: /127.0.0.1:50010
2019-10-10 16:17:32,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53724, dest: /127.0.0.1:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1127944426_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741917_1093, duration: 2721238
2019-10-10 16:17:32,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741917_1093, type=LAST_IN_PIPELINE terminating
2019-10-10 16:17:33,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741918_1094 src: /127.0.0.1:53726 dest: /127.0.0.1:50010
2019-10-10 16:17:33,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:53726, dest: /127.0.0.1:50010, bytes: 135341, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1127944426_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741918_1094, duration: 14309274
2019-10-10 16:17:33,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741918_1094, type=LAST_IN_PIPELINE terminating
2019-10-10 16:32:25,819 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741919_1095 src: /127.0.0.1:54070 dest: /127.0.0.1:50010
2019-10-10 16:32:25,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54070, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_903331654_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741919_1095, duration: 10316248
2019-10-10 16:32:25,834 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741919_1095, type=LAST_IN_PIPELINE terminating
2019-10-10 16:32:25,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741920_1096 src: /127.0.0.1:54072 dest: /127.0.0.1:50010
2019-10-10 16:32:25,948 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54072, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_903331654_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741920_1096, duration: 2354093
2019-10-10 16:32:25,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741920_1096, type=LAST_IN_PIPELINE terminating
2019-10-10 16:32:25,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741921_1097 src: /127.0.0.1:54074 dest: /127.0.0.1:50010
2019-10-10 16:32:26,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54074, dest: /127.0.0.1:50010, bytes: 59, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_903331654_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741921_1097, duration: 2598396
2019-10-10 16:32:26,002 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741921_1097, type=LAST_IN_PIPELINE terminating
2019-10-10 16:32:26,212 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741922_1098 src: /127.0.0.1:54076 dest: /127.0.0.1:50010
2019-10-10 16:32:26,224 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54076, dest: /127.0.0.1:50010, bytes: 135341, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_903331654_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741922_1098, duration: 2510612
2019-10-10 16:32:26,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741922_1098, type=LAST_IN_PIPELINE terminating
2019-10-10 16:43:07,552 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741923_1099 src: /127.0.0.1:54352 dest: /127.0.0.1:50010
2019-10-10 16:43:07,554 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver constructor. Cause is 
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createTmpFile(DatanodeUtil.java:66)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:301)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:861)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1461)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:210)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1254)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:720)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:166)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:103)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 16:43:07,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting CheckDiskError Thread
2019-10-10 16:43:07,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1876871648-127.0.1.1-1570716796840:blk_1073741923_1099 received exception java.io.IOException: No such file or directory
2019-10-10 16:43:07,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 16:43:07,656 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: H6:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:54352 dst: /127.0.0.1:50010
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method)
	at java.io.File.createNewFile(File.java:1012)
	at org.apache.hadoop.hdfs.server.datanode.DatanodeUtil.createTmpFile(DatanodeUtil.java:66)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.createRbwFile(BlockPoolSlice.java:301)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.createRbwFile(FsVolumeImpl.java:861)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.createRbw(FsDatasetImpl.java:1461)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.<init>(BlockReceiver.java:210)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.getBlockReceiver(DataXceiver.java:1254)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:720)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:166)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:103)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:288)
	at java.lang.Thread.run(Thread.java:748)
2019-10-10 16:47:02,541 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-10 16:47:05,975 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-10 16:47:05,977 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-10 17:01:11,703 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-
2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/
share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/
hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-10 17:01:11,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-10 17:01:12,022 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-10 17:01:12,337 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-10 17:01:12,394 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-10 17:01:12,394 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-10 17:01:12,400 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-10 17:01:12,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-10 17:01:12,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-10 17:01:12,421 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-10 17:01:12,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-10 17:01:12,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-10 17:01:12,504 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-10 17:01:12,511 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-10 17:01:12,524 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-10 17:01:12,529 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-10 17:01:12,531 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-10 17:01:12,531 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-10 17:01:12,531 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-10 17:01:12,543 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 40713
2019-10-10 17:01:12,543 INFO org.mortbay.log: jetty-6.1.26
2019-10-10 17:01:12,741 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:40713
2019-10-10 17:01:13,342 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-10 17:01:13,345 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-10 17:01:13,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-10 17:01:13,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-10 17:01:13,429 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-10 17:01:13,439 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-10 17:01:13,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-10 17:01:13,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-10 17:01:13,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-10 17:01:13,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-10 17:01:13,590 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-10 17:01:13,591 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-10 17:01:13,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-10 17:01:13,804 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-10 17:01:13,853 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 24582@H6
2019-10-10 17:01:13,854 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/luroz/hadoop/store/datanode is not formatted for namespace 1698661175. Formatting...
2019-10-10 17:01:13,856 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-6875fdea-a312-4170-acb1-7475a7969b79 for directory /home/luroz/hadoop/store/datanode
2019-10-10 17:01:14,058 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-10 17:01:14,058 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-10 17:01:14,059 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840 is not formatted for BP-1876871648-127.0.1.1-1570716796840. Formatting ...
2019-10-10 17:01:14,059 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1876871648-127.0.1.1-1570716796840 directory /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current
2019-10-10 17:01:14,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=null
2019-10-10 17:01:14,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 371f6362-a87a-4f59-9334-2e5eacb5c892
2019-10-10 17:01:14,299 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-6875fdea-a312-4170-acb1-7475a7969b79
2019-10-10 17:01:14,300 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-10 17:01:14,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-10 17:01:14,348 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-10 17:01:14,348 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 17:01:14,374 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 17:01:14,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 61ms
2019-10-10 17:01:14,436 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 88ms
2019-10-10 17:01:14,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-10 17:01:14,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-10 17:01:14,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 0ms
2019-10-10 17:01:14,438 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2019-10-10 17:01:14,440 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode
2019-10-10 17:01:14,446 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/10/19 7:21 PM with interval of 21600000ms
2019-10-10 17:01:14,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 371f6362-a87a-4f59-9334-2e5eacb5c892) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-10 17:01:14,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 371f6362-a87a-4f59-9334-2e5eacb5c892) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-10 17:01:14,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-10 17:01:14,514 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-6875fdea-a312-4170-acb1-7475a7969b79): finished scanning block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-10 17:01:14,562 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-6875fdea-a312-4170-acb1-7475a7969b79): no suitable block pools found to scan.  Waiting 1814399878 ms.
2019-10-10 17:01:14,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x43fb7e8d8a9a367d,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 39 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-10 17:01:14,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-11 09:46:37,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-11 09:46:37,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-11 09:46:37,798 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-11 09:46:38,442 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-11 09:46:38,509 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-11 09:46:38,509 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-11 09:46:38,516 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-11 09:46:38,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-11 09:46:38,526 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-11 09:46:38,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-11 09:46:38,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-11 09:46:38,647 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-11 09:46:38,751 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-11 09:46:38,760 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-11 09:46:38,777 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-11 09:46:38,791 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-11 09:46:38,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-11 09:46:38,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-11 09:46:38,794 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-11 09:46:38,880 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42515
2019-10-11 09:46:38,880 INFO org.mortbay.log: jetty-6.1.26
2019-10-11 09:46:39,024 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42515
2019-10-11 09:46:40,542 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-11 09:46:40,544 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-11 09:46:40,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-11 09:46:40,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-11 09:46:40,850 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-11 09:46:40,926 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-11 09:46:41,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-11 09:46:41,271 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-11 09:46:41,297 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-11 09:46:41,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-11 09:46:41,413 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-11 09:46:41,426 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-11 09:46:41,921 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-11 09:46:41,948 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-11 09:46:42,010 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/luroz/hadoop/store/datanode/in_use.lock acquired by nodename 8704@H6
2019-10-11 09:46:42,136 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1876871648-127.0.1.1-1570716796840
2019-10-11 09:46:42,136 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840
2019-10-11 09:46:42,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1698661175;bpid=BP-1876871648-127.0.1.1-1570716796840;lv=-57;nsInfo=lv=-63;cid=CID-da841a1a-f6b8-4cac-8191-df1c6f05b906;nsid=1698661175;c=1570716796840;bpid=BP-1876871648-127.0.1.1-1570716796840;dnuuid=43215549-03ef-4122-81db-88cb805594c7
2019-10-11 09:46:42,318 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-ef6c53c4-807b-4700-bec3-0a3d36e71023
2019-10-11 09:46:42,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/luroz/hadoop/store/datanode/current, StorageType: DISK
2019-10-11 09:46:42,333 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-11 09:46:42,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-11 09:46:42,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-11 09:46:42,343 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-11 09:46:42,452 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1876871648-127.0.1.1-1570716796840 on /home/luroz/hadoop/store/datanode/current: 109ms
2019-10-11 09:46:42,452 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1876871648-127.0.1.1-1570716796840: 111ms
2019-10-11 09:46:42,455 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current...
2019-10-11 09:46:42,455 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/replicas doesn't exist 
2019-10-11 09:46:42,463 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1876871648-127.0.1.1-1570716796840 on volume /home/luroz/hadoop/store/datanode/current: 8ms
2019-10-11 09:46:42,463 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 9ms
2019-10-11 09:46:42,560 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/home/luroz/hadoop/store/datanode, DS-ef6c53c4-807b-4700-bec3-0a3d36e71023): no suitable block pools found to scan.  Waiting 1754071880 ms.
2019-10-11 09:46:42,566 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/11/19 1:05 PM with interval of 21600000ms
2019-10-11 09:46:42,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-11 09:46:42,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1876871648-127.0.1.1-1570716796840 (Datanode Uuid 43215549-03ef-4122-81db-88cb805594c7) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-11 09:46:42,622 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-11 09:46:42,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe3cefe29fc7d1a29,  containing 1 storage report(s), of which we sent 1. The reports had 8 total blocks and used 1 RPC(s). This took 4 msec to generate and 72 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-11 09:46:42,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-11 10:01:37,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xe3cefe29fc7d1a2a,  containing 1 storage report(s), of which we sent 1. The reports had 8 total blocks and used 1 RPC(s). This took 1 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-11 10:01:37,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1876871648-127.0.1.1-1570716796840
2019-10-11 10:50:34,247 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741924_1100 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741924 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741925_1101 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741925 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741926_1102 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741926 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741927_1103 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741927 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741928_1104 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741928 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741929_1105 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741929 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741930_1106 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741930 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741931_1107 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741931 for deletion
2019-10-11 10:50:34,249 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741924_1100 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741924
2019-10-11 10:50:34,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741925_1101 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741925
2019-10-11 10:50:34,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741926_1102 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741926
2019-10-11 10:50:34,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741927_1103 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741927
2019-10-11 10:50:34,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741928_1104 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741928
2019-10-11 10:50:34,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741929_1105 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741929
2019-10-11 10:50:34,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741930_1106 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741930
2019-10-11 10:50:34,251 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1876871648-127.0.1.1-1570716796840 blk_1073741931_1107 file /home/luroz/hadoop/store/datanode/current/BP-1876871648-127.0.1.1-1570716796840/current/finalized/subdir0/subdir0/blk_1073741931
2019-10-11 11:00:38,608 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741932_1108 src: /127.0.0.1:54816 dest: /127.0.0.1:50010
2019-10-11 11:00:38,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54816, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330721856_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741932_1108, duration: 24812822
2019-10-11 11:00:38,872 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741932_1108, type=LAST_IN_PIPELINE terminating
2019-10-11 11:00:39,539 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741933_1109 src: /127.0.0.1:54818 dest: /127.0.0.1:50010
2019-10-11 11:00:39,547 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54818, dest: /127.0.0.1:50010, bytes: 592, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330721856_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741933_1109, duration: 5632096
2019-10-11 11:00:39,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741933_1109, type=LAST_IN_PIPELINE terminating
2019-10-11 11:00:39,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741934_1110 src: /127.0.0.1:54820 dest: /127.0.0.1:50010
2019-10-11 11:00:39,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54820, dest: /127.0.0.1:50010, bytes: 44, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330721856_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741934_1110, duration: 3448600
2019-10-11 11:00:39,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741934_1110, type=LAST_IN_PIPELINE terminating
2019-10-11 11:00:40,544 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1876871648-127.0.1.1-1570716796840:blk_1073741935_1111 src: /127.0.0.1:54822 dest: /127.0.0.1:50010
2019-10-11 11:00:40,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:54822, dest: /127.0.0.1:50010, bytes: 135348, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_330721856_1, offset: 0, srvID: 43215549-03ef-4122-81db-88cb805594c7, blockid: BP-1876871648-127.0.1.1-1570716796840:blk_1073741935_1111, duration: 7149933
2019-10-11 11:00:40,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1876871648-127.0.1.1-1570716796840:blk_1073741935_1111, type=LAST_IN_PIPELINE terminating
2019-10-11 11:02:54,635 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-11 11:02:58,635 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-11 11:02:59,347 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-11 11:02:59,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-11 11:38:25,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-11 11:38:25,175 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-11 11:38:25,500 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-11 11:38:25,904 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-11 11:38:25,957 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-11 11:38:25,957 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-11 11:38:25,962 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-11 11:38:25,963 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-11 11:38:25,966 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-11 11:38:25,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-11 11:38:25,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-11 11:38:25,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-11 11:38:26,081 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-11 11:38:26,088 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-11 11:38:26,101 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-11 11:38:26,106 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-11 11:38:26,107 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-11 11:38:26,108 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-11 11:38:26,108 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-11 11:38:26,120 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 42871
2019-10-11 11:38:26,120 INFO org.mortbay.log: jetty-6.1.26
2019-10-11 11:38:26,308 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42871
2019-10-11 11:38:26,779 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-11 11:38:26,782 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-11 11:38:26,806 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-11 11:38:26,807 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-11 11:38:26,873 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-11 11:38:26,884 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-11 11:38:26,928 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-11 11:38:26,938 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-11 11:38:26,944 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-11 11:38:26,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-11 11:38:26,964 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-11 11:38:26,965 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-11 11:38:27,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-11 11:38:27,177 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-11 11:38:27,242 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/in_use.lock acquired by nodename 13285@H6
2019-10-11 11:38:27,244 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode is not formatted for namespace 987000046. Formatting...
2019-10-11 11:38:27,245 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8 for directory /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode
2019-10-11 11:38:27,375 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1332724136-127.0.1.1-1570808272446
2019-10-11 11:38:27,376 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446
2019-10-11 11:38:27,376 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446 is not formatted for BP-1332724136-127.0.1.1-1570808272446. Formatting ...
2019-10-11 11:38:27,376 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1332724136-127.0.1.1-1570808272446 directory /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current
2019-10-11 11:38:27,460 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=987000046;bpid=BP-1332724136-127.0.1.1-1570808272446;lv=-57;nsInfo=lv=-63;cid=CID-00e7d5f5-8518-44e1-9fc7-410b8b1bfa18;nsid=987000046;c=1570808272446;bpid=BP-1332724136-127.0.1.1-1570808272446;dnuuid=null
2019-10-11 11:38:27,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 31b38c12-bce4-4b41-be03-4fd513a1f57c
2019-10-11 11:38:27,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8
2019-10-11 11:38:27,587 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-11 11:38:27,600 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-11 11:38:27,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-11 11:38:27,607 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1332724136-127.0.1.1-1570808272446
2019-10-11 11:38:27,608 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-10-11 11:38:27,637 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1332724136-127.0.1.1-1570808272446 on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 30ms
2019-10-11 11:38:27,638 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1332724136-127.0.1.1-1570808272446: 31ms
2019-10-11 11:38:27,640 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-10-11 11:38:27,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/replicas doesn't exist 
2019-10-11 11:38:27,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 0ms
2019-10-11 11:38:27,641 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2019-10-11 11:38:27,643 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode
2019-10-11 11:38:27,650 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/11/19 1:54 PM with interval of 21600000ms
2019-10-11 11:38:27,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-11 11:38:27,689 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8): finished scanning block pool BP-1332724136-127.0.1.1-1570808272446
2019-10-11 11:38:27,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-11 11:38:27,704 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-11 11:38:27,726 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8): no suitable block pools found to scan.  Waiting 1814399917 ms.
2019-10-11 11:38:27,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x279a337459fdad29,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 50 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-11 11:38:27,827 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-10-11 11:42:49,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741825_1001 src: /127.0.0.1:55632 dest: /127.0.0.1:50010
2019-10-11 11:45:05,710 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55632, dest: /127.0.0.1:50010, bytes: 539236, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1186547226_28, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741825_1001, duration: 135861304773
2019-10-11 11:45:05,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-10-11 11:47:19,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741826_1002 src: /127.0.0.1:55722 dest: /127.0.0.1:50010
2019-10-11 11:47:19,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55722, dest: /127.0.0.1:50010, bytes: 28094, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1195676730_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741826_1002, duration: 8790455
2019-10-11 11:47:19,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2019-10-11 12:01:02,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741827_1003 src: /127.0.0.1:55936 dest: /127.0.0.1:50010
2019-10-11 12:01:02,151 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55936, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1761656017_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741827_1003, duration: 9774301
2019-10-11 12:01:02,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-10-11 12:01:02,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741828_1004 src: /127.0.0.1:55938 dest: /127.0.0.1:50010
2019-10-11 12:01:02,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55938, dest: /127.0.0.1:50010, bytes: 124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1761656017_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741828_1004, duration: 1714821
2019-10-11 12:01:02,348 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2019-10-11 12:01:02,394 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741829_1005 src: /127.0.0.1:55940 dest: /127.0.0.1:50010
2019-10-11 12:01:02,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55940, dest: /127.0.0.1:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1761656017_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741829_1005, duration: 3730197
2019-10-11 12:01:02,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2019-10-11 12:01:03,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741830_1006 src: /127.0.0.1:55942 dest: /127.0.0.1:50010
2019-10-11 12:01:03,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:55942, dest: /127.0.0.1:50010, bytes: 135553, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1761656017_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741830_1006, duration: 3160972
2019-10-11 12:01:03,282 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2019-10-11 13:39:11,059 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "H6/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-10-11 13:39:14,108 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-11 13:39:14,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-12 10:58:03,072 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-12 10:58:03,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-12 10:58:03,449 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 10:58:04,099 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode : 
java.io.FileNotFoundException: File file:/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:635)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:861)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:625)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:233)
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2580)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2622)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-10-12 10:58:04,102 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2631)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-10-12 10:58:04,104 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2019-10-12 10:58:04,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-10-12 11:00:18,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-12 11:00:18,031 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-12 11:00:18,346 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-12 11:00:18,620 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-12 11:00:18,678 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-10-12 11:00:18,678 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-10-12 11:00:18,697 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-10-12 11:00:18,698 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-10-12 11:00:18,701 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-10-12 11:00:18,719 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-10-12 11:00:18,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-10-12 11:00:18,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-10-12 11:00:18,839 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-12 11:00:18,845 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-12 11:00:18,858 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-10-12 11:00:18,864 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-12 11:00:18,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-10-12 11:00:18,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-12 11:00:18,866 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-12 11:00:18,897 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39679
2019-10-12 11:00:18,897 INFO org.mortbay.log: jetty-6.1.26
2019-10-12 11:00:19,071 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39679
2019-10-12 11:00:19,613 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-10-12 11:00:19,615 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-10-12 11:00:19,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-10-12 11:00:19,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-10-12 11:00:19,833 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-10-12 11:00:19,844 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-10-12 11:00:19,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-10-12 11:00:19,898 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-10-12 11:00:19,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-10-12 11:00:19,935 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-10-12 11:00:19,944 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-10-12 11:00:19,946 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-10-12 11:00:20,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-10-12 11:00:20,221 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-10-12 11:00:20,310 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/in_use.lock acquired by nodename 8289@H6
2019-10-12 11:00:20,423 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1332724136-127.0.1.1-1570808272446
2019-10-12 11:00:20,424 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446
2019-10-12 11:00:20,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=987000046;bpid=BP-1332724136-127.0.1.1-1570808272446;lv=-57;nsInfo=lv=-63;cid=CID-00e7d5f5-8518-44e1-9fc7-410b8b1bfa18;nsid=987000046;c=1570808272446;bpid=BP-1332724136-127.0.1.1-1570808272446;dnuuid=31b38c12-bce4-4b41-be03-4fd513a1f57c
2019-10-12 11:00:20,571 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8
2019-10-12 11:00:20,572 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current, StorageType: DISK
2019-10-12 11:00:20,577 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-10-12 11:00:20,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-10-12 11:00:20,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1332724136-127.0.1.1-1570808272446
2019-10-12 11:00:20,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-10-12 11:00:20,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1332724136-127.0.1.1-1570808272446 on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 168ms
2019-10-12 11:00:20,752 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1332724136-127.0.1.1-1570808272446: 169ms
2019-10-12 11:00:20,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-10-12 11:00:20,756 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/replicas doesn't exist 
2019-10-12 11:00:20,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 6ms
2019-10-12 11:00:20,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 7ms
2019-10-12 11:00:20,877 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8): no suitable block pools found to scan.  Waiting 1730286766 ms.
2019-10-12 11:00:20,892 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 10/12/19 12:11 PM with interval of 21600000ms
2019-10-12 11:00:20,894 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-10-12 11:00:20,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-10-12 11:00:20,939 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-10-12 11:00:21,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x35943401858e20a6,  containing 1 storage report(s), of which we sent 1. The reports had 6 total blocks and used 1 RPC(s). This took 4 msec to generate and 49 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-12 11:00:21,090 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-10-12 11:17:27,006 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-10-12 11:17:27,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2019-10-12 11:17:27,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-10-12 11:17:27,017 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2019-10-12 11:17:27,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1332724136-127.0.1.1-1570808272446 blk_1073741827_1003 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741827
2019-10-12 11:17:27,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1332724136-127.0.1.1-1570808272446 blk_1073741828_1004 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741828
2019-10-12 11:17:27,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1332724136-127.0.1.1-1570808272446 blk_1073741829_1005 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741829
2019-10-12 11:17:27,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1332724136-127.0.1.1-1570808272446 blk_1073741830_1006 file /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/finalized/subdir0/subdir0/blk_1073741830
2019-10-12 11:26:51,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741831_1007 src: /127.0.0.1:43888 dest: /127.0.0.1:50010
2019-10-12 11:26:51,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43888, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1024502030_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741831_1007, duration: 10397123
2019-10-12 11:26:51,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2019-10-12 11:26:52,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741832_1008 src: /127.0.0.1:43890 dest: /127.0.0.1:50010
2019-10-12 11:26:52,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43890, dest: /127.0.0.1:50010, bytes: 124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1024502030_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741832_1008, duration: 6899388
2019-10-12 11:26:52,653 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2019-10-12 11:26:52,837 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741833_1009 src: /127.0.0.1:43892 dest: /127.0.0.1:50010
2019-10-12 11:26:52,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43892, dest: /127.0.0.1:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1024502030_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741833_1009, duration: 3654079
2019-10-12 11:26:52,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2019-10-12 11:26:54,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741834_1010 src: /127.0.0.1:43894 dest: /127.0.0.1:50010
2019-10-12 11:26:54,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43894, dest: /127.0.0.1:50010, bytes: 135555, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1024502030_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741834_1010, duration: 8582491
2019-10-12 11:26:54,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2019-10-12 11:27:08,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741835_1011 src: /127.0.0.1:43916 dest: /127.0.0.1:50010
2019-10-12 11:27:08,389 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43916, dest: /127.0.0.1:50010, bytes: 157610, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2056317419_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741835_1011, duration: 16728904
2019-10-12 11:27:08,391 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-10-12 11:56:17,666 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x35943401858e20a7,  containing 1 storage report(s), of which we sent 1. The reports had 7 total blocks and used 1 RPC(s). This took 0 msec to generate and 55 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-12 11:56:17,669 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-10-12 12:11:40,519 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1332724136-127.0.1.1-1570808272446 Total blocks: 7, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-10-12 17:56:15,788 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x35943401858e20a8,  containing 1 storage report(s), of which we sent 1. The reports had 7 total blocks and used 1 RPC(s). This took 0 msec to generate and 75 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-10-12 17:56:15,797 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-10-12 18:11:40,923 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1332724136-127.0.1.1-1570808272446 Total blocks: 7, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-10-12 20:46:05,331 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1142ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1222ms
2019-11-14 18:36:39,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-11-14 18:36:39,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-14 18:36:40,035 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-11-14 18:36:40,881 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-14 18:36:40,941 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-11-14 18:36:40,941 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-14 18:36:41,014 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-11-14 18:36:41,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-11-14 18:36:41,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-14 18:36:41,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-14 18:36:41,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-11-14 18:36:41,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-11-14 18:36:41,506 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-14 18:36:41,516 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-11-14 18:36:41,531 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-14 18:36:41,537 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-14 18:36:41,539 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-14 18:36:41,539 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-14 18:36:41,539 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-14 18:36:41,619 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39617
2019-11-14 18:36:41,620 INFO org.mortbay.log: jetty-6.1.26
2019-11-14 18:36:41,934 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39617
2019-11-14 18:36:43,380 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-11-14 18:36:43,384 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-11-14 18:36:43,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-11-14 18:36:43,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-14 18:36:43,937 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-11-14 18:36:44,066 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-14 18:36:44,712 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-14 18:36:44,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-14 18:36:44,732 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-14 18:36:44,811 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-11-14 18:36:44,874 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-14 18:36:44,874 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-14 18:36:46,616 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-11-14 18:36:47,156 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-11-14 18:36:47,158 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-11-14 18:36:47,289 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/in_use.lock acquired by nodename 5007@H6
2019-11-14 18:36:47,551 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1332724136-127.0.1.1-1570808272446
2019-11-14 18:36:47,551 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446
2019-11-14 18:36:47,814 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=987000046;bpid=BP-1332724136-127.0.1.1-1570808272446;lv=-57;nsInfo=lv=-63;cid=CID-00e7d5f5-8518-44e1-9fc7-410b8b1bfa18;nsid=987000046;c=1570808272446;bpid=BP-1332724136-127.0.1.1-1570808272446;dnuuid=31b38c12-bce4-4b41-be03-4fd513a1f57c
2019-11-14 18:36:47,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8
2019-11-14 18:36:47,882 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current, StorageType: DISK
2019-11-14 18:36:47,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-14 18:36:47,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-11-14 18:36:47,892 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-14 18:36:47,895 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-11-14 18:36:48,019 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1332724136-127.0.1.1-1570808272446 on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 124ms
2019-11-14 18:36:48,019 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1332724136-127.0.1.1-1570808272446: 127ms
2019-11-14 18:36:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-11-14 18:36:48,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/replicas doesn't exist 
2019-11-14 18:36:48,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 4ms
2019-11-14 18:36:48,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-11-14 18:36:48,105 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now rescanning bpid BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, after more than 504 hour(s)
2019-11-14 18:36:48,111 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/14/19 8:29 PM with interval of 21600000ms
2019-11-14 18:36:48,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-11-14 18:36:48,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-11-14 18:36:48,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-14 18:36:48,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x34df6f1c4ae648aa,  containing 1 storage report(s), of which we sent 1. The reports had 7 total blocks and used 1 RPC(s). This took 3 msec to generate and 42 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-14 18:36:48,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-14 18:36:49,336 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8): finished scanning block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-14 18:36:49,394 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8): no suitable block pools found to scan.  Waiting 1814398711 ms.
2019-11-14 18:58:13,484 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1566ms
No GCs detected
2019-11-14 19:08:35,530 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-11-14 19:08:38,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-11-15 10:54:02,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-11-15 10:54:02,588 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-15 10:54:02,945 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-11-15 10:54:03,297 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode : 
java.io.FileNotFoundException: File file:/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:635)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:861)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:625)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:233)
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2580)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2622)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-11-15 10:54:03,299 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2631)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-11-15 10:54:03,300 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2019-11-15 10:54:03,302 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-11-15 10:56:12,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-11-15 10:56:12,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-15 10:56:12,393 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-11-15 10:56:12,644 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode : 
java.io.FileNotFoundException: File file:/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:635)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:861)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:625)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:233)
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2580)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2622)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-11-15 10:56:12,646 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2631)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-11-15 10:56:12,647 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2019-11-15 10:56:12,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-11-15 10:58:57,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-11-15 10:58:57,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-15 10:58:57,400 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-11-15 10:58:57,665 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-15 10:58:57,721 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-11-15 10:58:57,721 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-15 10:58:57,741 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-11-15 10:58:57,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-11-15 10:58:57,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-15 10:58:57,761 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-15 10:58:57,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-11-15 10:58:57,763 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-11-15 10:58:57,830 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-15 10:58:57,836 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-11-15 10:58:57,849 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-15 10:58:57,855 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-15 10:58:57,857 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-15 10:58:57,857 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-15 10:58:57,857 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-15 10:58:57,881 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 35287
2019-11-15 10:58:57,881 INFO org.mortbay.log: jetty-6.1.26
2019-11-15 10:58:57,994 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35287
2019-11-15 10:58:58,419 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-11-15 10:58:58,421 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-11-15 10:58:58,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-11-15 10:58:58,446 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-15 10:58:58,621 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-11-15 10:58:58,633 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-15 10:58:58,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-15 10:58:58,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-15 10:58:58,693 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-15 10:58:58,799 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-11-15 10:58:58,818 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-15 10:58:58,818 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-15 10:58:59,005 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-11-15 10:58:59,007 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-11-15 10:58:59,047 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/in_use.lock acquired by nodename 7421@H6
2019-11-15 10:58:59,180 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1332724136-127.0.1.1-1570808272446
2019-11-15 10:58:59,180 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446
2019-11-15 10:58:59,238 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=987000046;bpid=BP-1332724136-127.0.1.1-1570808272446;lv=-57;nsInfo=lv=-63;cid=CID-00e7d5f5-8518-44e1-9fc7-410b8b1bfa18;nsid=987000046;c=1570808272446;bpid=BP-1332724136-127.0.1.1-1570808272446;dnuuid=31b38c12-bce4-4b41-be03-4fd513a1f57c
2019-11-15 10:58:59,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8
2019-11-15 10:58:59,314 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current, StorageType: DISK
2019-11-15 10:58:59,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-15 10:58:59,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-11-15 10:58:59,325 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-15 10:58:59,327 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-11-15 10:58:59,399 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1332724136-127.0.1.1-1570808272446 on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 72ms
2019-11-15 10:58:59,400 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1332724136-127.0.1.1-1570808272446: 74ms
2019-11-15 10:58:59,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-11-15 10:58:59,402 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/replicas doesn't exist 
2019-11-15 10:58:59,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 5ms
2019-11-15 10:58:59,406 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2019-11-15 10:58:59,479 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8): no suitable block pools found to scan.  Waiting 1755468626 ms.
2019-11-15 10:58:59,485 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/15/19 1:43 PM with interval of 21600000ms
2019-11-15 10:58:59,487 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-11-15 10:58:59,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-11-15 10:58:59,525 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-15 10:58:59,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc2f406b2ca96d3ca,  containing 1 storage report(s), of which we sent 1. The reports had 7 total blocks and used 1 RPC(s). This took 5 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-15 10:58:59,674 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-15 11:07:22,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741836_1012 src: /127.0.0.1:51624 dest: /127.0.0.1:50010
2019-11-15 11:07:23,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:51624, dest: /127.0.0.1:50010, bytes: 16432841, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1402571223_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741836_1012, duration: 135243471
2019-11-15 11:07:23,234 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741836_1012, type=LAST_IN_PIPELINE terminating
2019-11-15 12:10:21,143 INFO logs: Aliases are enabled
2019-11-15 12:17:11,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741837_1013 src: /127.0.0.1:52136 dest: /127.0.0.1:50010
2019-11-15 12:17:11,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52136, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-118772934_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741837_1013, duration: 12655022
2019-11-15 12:17:11,830 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741837_1013, type=LAST_IN_PIPELINE terminating
2019-11-15 12:17:12,102 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741838_1014 src: /127.0.0.1:52138 dest: /127.0.0.1:50010
2019-11-15 12:17:12,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52138, dest: /127.0.0.1:50010, bytes: 124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-118772934_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741838_1014, duration: 3183398
2019-11-15 12:17:12,107 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741838_1014, type=LAST_IN_PIPELINE terminating
2019-11-15 12:17:12,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741839_1015 src: /127.0.0.1:52140 dest: /127.0.0.1:50010
2019-11-15 12:17:12,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52140, dest: /127.0.0.1:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-118772934_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741839_1015, duration: 2474500
2019-11-15 12:17:12,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741839_1015, type=LAST_IN_PIPELINE terminating
2019-11-15 12:17:13,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741840_1016 src: /127.0.0.1:52142 dest: /127.0.0.1:50010
2019-11-15 12:17:13,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52142, dest: /127.0.0.1:50010, bytes: 135556, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-118772934_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741840_1016, duration: 4749434
2019-11-15 12:17:13,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741840_1016, type=LAST_IN_PIPELINE terminating
2019-11-15 12:18:06,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741841_1017 src: /127.0.0.1:52170 dest: /127.0.0.1:50010
2019-11-15 12:18:06,289 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52170, dest: /127.0.0.1:50010, bytes: 157611, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1595696784_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741841_1017, duration: 9508109
2019-11-15 12:18:06,290 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741841_1017, type=LAST_IN_PIPELINE terminating
2019-11-15 12:39:57,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741842_1018 src: /127.0.0.1:52342 dest: /127.0.0.1:50010
2019-11-15 12:39:57,216 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52342, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1405870135_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741842_1018, duration: 12940357
2019-11-15 12:39:57,217 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741842_1018, type=LAST_IN_PIPELINE terminating
2019-11-15 12:39:57,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741843_1019 src: /127.0.0.1:52344 dest: /127.0.0.1:50010
2019-11-15 12:39:57,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52344, dest: /127.0.0.1:50010, bytes: 124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1405870135_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741843_1019, duration: 58762338
2019-11-15 12:39:57,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741843_1019, type=LAST_IN_PIPELINE terminating
2019-11-15 12:39:58,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741844_1020 src: /127.0.0.1:52346 dest: /127.0.0.1:50010
2019-11-15 12:39:58,162 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52346, dest: /127.0.0.1:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1405870135_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741844_1020, duration: 1921814
2019-11-15 12:39:58,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741844_1020, type=LAST_IN_PIPELINE terminating
2019-11-15 12:39:59,628 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741845_1021 src: /127.0.0.1:52348 dest: /127.0.0.1:50010
2019-11-15 12:39:59,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:52348, dest: /127.0.0.1:50010, bytes: 135556, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1405870135_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741845_1021, duration: 3212281
2019-11-15 12:39:59,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741845_1021, type=LAST_IN_PIPELINE terminating
2019-11-15 12:58:16,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xc2f406b2ca96d3cb,  containing 1 storage report(s), of which we sent 1. The reports had 17 total blocks and used 1 RPC(s). This took 35 msec to generate and 369 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-15 12:58:16,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-15 13:37:46,010 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-11-15 13:37:46,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-15 13:37:46,390 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-11-15 13:37:46,765 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode : 
java.io.FileNotFoundException: File file:/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:635)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:861)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:625)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:233)
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:2580)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2622)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-11-15 13:37:46,768 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:2631)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2604)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2497)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2544)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2753)
2019-11-15 13:37:46,770 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2019-11-15 13:37:46,774 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at H6/127.0.1.1
************************************************************/
2019-11-15 13:38:40,770 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = luroz
STARTUP_MSG:   host = H6/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/luroz/hadoop/etc/hadoop:/home/luroz/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/home/luroz/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/luroz/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/luroz/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs:/home/luroz/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/luroz/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/luroz/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/luroz/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-11-15 13:38:40,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-11-15 13:38:41,100 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-11-15 13:38:41,409 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-11-15 13:38:41,460 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-11-15 13:38:41,461 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-11-15 13:38:41,494 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-11-15 13:38:41,495 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is H6
2019-11-15 13:38:41,498 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-11-15 13:38:41,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-11-15 13:38:41,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-11-15 13:38:41,517 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-11-15 13:38:41,588 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-11-15 13:38:41,594 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-11-15 13:38:41,609 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-11-15 13:38:41,614 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-11-15 13:38:41,616 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-11-15 13:38:41,616 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-11-15 13:38:41,616 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-11-15 13:38:41,628 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 39551
2019-11-15 13:38:41,628 INFO org.mortbay.log: jetty-6.1.26
2019-11-15 13:38:41,743 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:39551
2019-11-15 13:38:42,010 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-11-15 13:38:42,012 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-11-15 13:38:42,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = luroz
2019-11-15 13:38:42,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-11-15 13:38:42,120 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-11-15 13:38:42,131 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-11-15 13:38:42,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-11-15 13:38:42,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-11-15 13:38:42,191 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-11-15 13:38:42,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-11-15 13:38:42,232 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-11-15 13:38:42,233 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-11-15 13:38:42,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-11-15 13:38:42,510 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-11-15 13:38:42,573 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/in_use.lock acquired by nodename 4145@H6
2019-11-15 13:38:42,764 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1332724136-127.0.1.1-1570808272446
2019-11-15 13:38:42,764 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446
2019-11-15 13:38:42,824 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=987000046;bpid=BP-1332724136-127.0.1.1-1570808272446;lv=-57;nsInfo=lv=-63;cid=CID-00e7d5f5-8518-44e1-9fc7-410b8b1bfa18;nsid=987000046;c=1570808272446;bpid=BP-1332724136-127.0.1.1-1570808272446;dnuuid=31b38c12-bce4-4b41-be03-4fd513a1f57c
2019-11-15 13:38:42,925 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8
2019-11-15 13:38:42,925 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current, StorageType: DISK
2019-11-15 13:38:42,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-11-15 13:38:42,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-11-15 13:38:42,935 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-15 13:38:42,936 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-11-15 13:38:43,012 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1332724136-127.0.1.1-1570808272446 on /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 75ms
2019-11-15 13:38:43,012 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1332724136-127.0.1.1-1570808272446: 77ms
2019-11-15 13:38:43,014 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current...
2019-11-15 13:38:43,014 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current/BP-1332724136-127.0.1.1-1570808272446/current/replicas doesn't exist 
2019-11-15 13:38:43,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1332724136-127.0.1.1-1570808272446 on volume /media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode/current: 7ms
2019-11-15 13:38:43,021 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2019-11-15 13:38:43,107 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/media/luroz/b00b357b-1dac-4b88-98fa-3f2cd80fa803/luroz/hadoop/datanode, DS-3a37c8c3-6b1f-4848-a33a-4cfaa2264cc8): no suitable block pools found to scan.  Waiting 1745884998 ms.
2019-11-15 13:38:43,112 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 11/15/19 2:08 PM with interval of 21600000ms
2019-11-15 13:38:43,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-11-15 13:38:43,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1332724136-127.0.1.1-1570808272446 (Datanode Uuid 31b38c12-bce4-4b41-be03-4fd513a1f57c) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-11-15 13:38:43,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-11-15 13:38:43,306 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0xed0d434fe62324c8,  containing 1 storage report(s), of which we sent 1. The reports had 17 total blocks and used 1 RPC(s). This took 4 msec to generate and 41 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-11-15 13:38:43,307 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1332724136-127.0.1.1-1570808272446
2019-11-15 13:53:23,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741846_1022 src: /127.0.0.1:37394 dest: /127.0.0.1:50010
2019-11-15 13:53:23,916 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37394, dest: /127.0.0.1:50010, bytes: 5162, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-240939221_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741846_1022, duration: 9670127
2019-11-15 13:53:23,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741846_1022, type=LAST_IN_PIPELINE terminating
2019-11-15 13:53:24,821 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741847_1023 src: /127.0.0.1:37396 dest: /127.0.0.1:50010
2019-11-15 13:53:24,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37396, dest: /127.0.0.1:50010, bytes: 124, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-240939221_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741847_1023, duration: 2329234
2019-11-15 13:53:24,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741847_1023, type=LAST_IN_PIPELINE terminating
2019-11-15 13:53:25,030 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741848_1024 src: /127.0.0.1:37398 dest: /127.0.0.1:50010
2019-11-15 13:53:25,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37398, dest: /127.0.0.1:50010, bytes: 19, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-240939221_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741848_1024, duration: 2065602
2019-11-15 13:53:25,035 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741848_1024, type=LAST_IN_PIPELINE terminating
2019-11-15 13:53:26,071 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741849_1025 src: /127.0.0.1:37400 dest: /127.0.0.1:50010
2019-11-15 13:53:26,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37400, dest: /127.0.0.1:50010, bytes: 135556, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-240939221_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741849_1025, duration: 12271035
2019-11-15 13:53:26,086 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741849_1025, type=LAST_IN_PIPELINE terminating
2019-11-15 13:53:39,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1332724136-127.0.1.1-1570808272446:blk_1073741850_1026 src: /127.0.0.1:37420 dest: /127.0.0.1:50010
2019-11-15 13:53:39,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37420, dest: /127.0.0.1:50010, bytes: 157611, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1406724427_1, offset: 0, srvID: 31b38c12-bce4-4b41-be03-4fd513a1f57c, blockid: BP-1332724136-127.0.1.1-1570808272446:blk_1073741850_1026, duration: 8870299
2019-11-15 13:53:39,657 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1332724136-127.0.1.1-1570808272446:blk_1073741850_1026, type=LAST_IN_PIPELINE terminating
2019-11-15 14:08:04,194 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1332724136-127.0.1.1-1570808272446 Total blocks: 22, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
